---
title: "lrti_appro_model_2019"
author: "Billy"
date: "18/05/2022"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = '/workspace')
```


```{r}
library("dplyr")
library("tidyverse")
library("lubridate")
library("foreign")
library("nnet")
library("ggplot2")
library("VGAM")
library("bayesm")
library("finalfit")
library("here")

```

```{r}
DF <- read_csv(here::here("output","lrti_outcome_2019.csv"))
DF <- DF %>% filter (age>3)
DF$outcome <- as.factor(DF$outcome)
DF$ethnicity_6 <- as.factor(DF$ethnicity_6)
DF$imd <- as.factor(DF$imd)
DF$region <- as.factor(DF$region)
DF$charlsonGrp <- as.factor(DF$charlsonGrp)
DF$ab12b4 <- as.factor(DF$ab12b4)
DF$outcome <- relevel(DF$outcome, ref = "1")
DF$ethnicity_6 <- relevel(DF$ethnicity_6, ref = "White")
DF$imd <- relevel(DF$imd, ref = "1")
DF$region <- relevel(DF$region, ref = "East")
DF$charlsonGrp <- relevel(DF$charlsonGrp, ref = "zero")
DF$ab12b4 <- relevel(DF$ab12b4, ref = "0")
DF <- DF %>% dplyr::select(outcome,age,sex,ethnicity_6,region,charlsonGrp,imd,ab12b4)
DF <- DF %>% filter (DF$sex=="M"|DF$sex=="F")
DF <- DF %>% filter (!is.na(outcome))
DF <- DF %>% filter (!is.na(ethnicity_6))
DF <- DF %>% filter (!is.na(imd))
DF <- DF %>% filter (!is.na(region))
DF <- DF %>% filter (!is.na(charlsonGrp))
DF <- DF %>% filter (!is.na(age))
DF <- DF %>% filter (!is.na(sex))
DF <- DF %>% filter (!is.na(ab12b4))
```

```{r}

# Data Partition 
set.seed(777)
ind <- sample(2,nrow(DF),
              replace = TRUE,
              prob = c(0.75,0.25))

training <- DF[ind==1,]
testing <- DF[ind==2,]

### use training dataset to develop the model
m0 <- multinom(outcome ~ age + sex ,data = training)
m1 <- multinom(outcome ~.,data = training)
fit.multinomial <- vgam(outcome ~., family = multinomial(ref = "1"), data = training)
```

```{r}
anova(m0,m1)
```

```{r}
### 2-tailed Z-test
z <- summary(m1)$coefficients/summary(m1)$standard.errors
p <- (1 - pnorm(abs(z),0,1)) * 2
p
```


```{r}
###relative risk ratio ###
exp(coef(m1))
```


```{r}
### Confusion Matrix & Misclassification Error - Training Data
p <- predict(m1,training)
tab <- table(p, training$outcome)
tab
```

```{r}
sum(diag(tab))/sum(tab)
```

```{r}
1-sum(diag(tab))/sum(tab)
```

```{r}
### Confusion Matrix & Misclassification Error - Testing data
p1 <- predict(m1,testing)
tab1 <- table(p1, testing$outcome)
tab1
```

```{r}
1-sum(diag(tab1))/sum(tab1)
```


```{r}
### Table 1. Description and descriptive statistics for the case studies for each outcome category separately.
# columns for  table
colsfortab <- colnames(training)
training %>% summary_factorlist(explanatory = colsfortab) -> t1
t1
```

```{r}
testing %>% summary_factorlist(explanatory = colsfortab) -> t2
t2
```

```{r}
### Table 2. Assessment of calibration-in-the-large in the validation data.
n <- table(testing$outcome)
n/sum(n)
```

```{r}
tab1/colSums(tab1)
```


### Parametric nominal calibration plot for the validation data of the ovarian tumor case study for each outcome category separately ((a)â€“(c)) and overall (d).

```{r}
outcome=testing$outcome
k=3
p <- predict(fit.multinomial , newdata = testing,type="response")
LP <- predict(fit.multinomial, newdata = testing)
r=1
estimates=FALSE
dfr=2
plotoverall=TRUE
datapoints=TRUE
smoothing=TRUE
smoothpar=1
intercept=FALSE
slope=FALSE
test=FALSE
  # probabilities
  probs <- split(p,col(p))    
  
  # linear predictors necessary for non-parametric calibration plot - give a name to each linear predictor 
  # seperately
  lps <- split(LP,col(LP))
  for(i in 1:(k-1)){assign(paste("lp", i, sep = ""),unlist(lps[[i]]))}
aaa <- length(LP[,1])
bbb <- length(outcome)
cbind(aaa,bbb)
```


```{r}
  ###############################################
  # parametric logistic recalibration framework 
  # cf. section 2.2.1.                          
  ###############################################
  
  # reference category r
  # LP = matrix with linear predictors


  fitp<-vglm(outcome~LP,family=multinomial(refLevel=r))
  if(isTRUE(estimates)){est<-coefficients(fitp)
  names(est) <- paste('EST',names(est),sep='.')}
  
```





```{r}
  
  fitnp<-vgam(outcome~s(lp1,df=dfr)+s(lp2,df=dfr),family=multinomial(refLevel=r))
  
  
  ###############################################                  
  # Separate (non-)parametric calibration plots
  ###############################################
  
  
  par(mfrow=c(ceiling(k/2),2))
  for(i in 1:k){p <- unlist(probs[[i]])
  if(isTRUE(smoothing)){color<-'grey'}else{color<-1+i}
 
  matplot(p,fitted(fitp)[,i],type="p",pch=i,col=color,lwd=1,ylab="",xlab="",xlim=0:1,ylim=0:1)
  par(new=T)
  ref <- rbind(c(0,0),c(1,1))
  matplot(ref,ref,type="l",col=1,lwd=2,ylab="Observed proportions",xlab="Predicted probabilities",xlim=0:1,ylim=0:1)
  # smoother for calibration plots 
  ##################################
  # a = smoothing parameter
  if(isTRUE(smoothing)){
    a = smoothpar
    points(smooth.spline(p, fitted(fitp)[,i],spar=a), type="l", col=(1+i), lwd = 4)}
  # legend
  legende <- c(paste("outcome ", i, sep = ""))
  legend(x=0.6, y=(0.2),col=(1+i),lty =1,legend=legende)
  title(main = "Parametric calibration plot")
  par(new=F)} 
```


```{r}
  
  # non-parametric calibration plot 
  # cf. section 2.2.2.              
  ###################################
  
  par(mfrow=c(ceiling(k/2),2))
  for(i in 1:k){p <- unlist(probs[[i]])
  if(isTRUE(smoothing)){color<-'grey'}else{color<-1+i}
  plot2 <- matplot(p,fitted(fitnp)[,i],type="p",pch=i,col=color,lwd=1,ylab="",xlab="",xlim=0:1,ylim=0:1)
  par(new=T)
  ref <- rbind(c(0,0),c(1,1))
  matplot(ref,ref,type="l",col=1,lwd=2,ylab="Observed proportions",xlab="Predicted probabilities",xlim=0:1,ylim=0:1)
  # smoother for calibration plots 
  ##################################
  # a = smoothing parameter
  if(isTRUE(smoothing)){
    a = smoothpar
    points(smooth.spline(p, fitted(fitnp)[,i],spar=a), type="l", col=(1+i), lwd = 4)}
  # legend
  legende <- c(paste("cat ", i, sep = ""))
  legend(x=0.6, y=(0.2),col=(1+i),lty =1,legend=legende)
  title(main = "Non-parametric calibration plot")
  par(new=F)}
  
```



```{r}
  if(isTRUE(plotoverall)){
    
    
    # parametric calibration plot 
    # cf. section 2.2.2.          
    ###############################
    
    if(isTRUE(datapoints)){for(i in 1:k){p <- unlist(probs[[i]])
    plot3 <- matplot(p,fitted(fitp)[,i],type="p",pch=i,col=(1+i),lwd=1,ylab="",xlab="",xlim=0:1,ylim=0:1)
    par(new=T)}}
    ref <- rbind(c(0,0),c(1,1))
    matplot(ref,ref,type="l",col=1,lwd=2,ylab="Observed proportions",xlab="Predicted probabilities",xlim=0:1,ylim=0:1)
    # smoother for calibration plots 
    ##################################
    # a = smoothing parameter
    if(isTRUE(smoothing)){
      a = smoothpar
      for(i in 1:k){p <- unlist(probs[[i]])
      points(smooth.spline(p, fitted(fitp)[,i],spar=a), type="l", col=(1+i), lwd = 4)}}
    # legend
    for(i in 1:k){if(i <= 2){legende <- c("cat 1","cat 2")}
      if(i > 2){legende <- c(legende,paste("cat ", i, sep = ""))}}
    legend(x=0.7, y=(0.20+(k-3)*0.05),col=2:(k+1),lty =1,legend=legende)
    title(main = "Parametric calibration plot")
    par(new=F)
    
    # non-parametric calibration plot 
    # cf. section 2.2.2.              
    ###################################
    
    
    if(isTRUE(datapoints)){for(i in 1:k){p <- unlist(probs[[i]])
    plot4 <- matplot(p,fitted(fitnp)[,i],type="p",pch=i,col=(1+i),lwd=1,ylab="",xlab="",xlim=0:1,ylim=0:1)
    par(new=T)}}
    ref <- rbind(c(0,0),c(1,1))
    matplot(ref,ref,type="l",col=1,lwd=2,ylab="Observed proportions",xlab="Predicted  probabilities",xlim=0:1,ylim=0:1)
    # smoother for calibration plots 
    ##################################
    # a = smoothing parameter
    if(isTRUE(smoothing)){a = smoothpar
    for(i in 1:k){p <- unlist(probs[[i]])
    points(smooth.spline(p, fitted(fitnp)[,i],spar=a), type="l", col=(1+i), lwd = 4)}}
    # legend
    for(i in 1:k){if(i <= 2){legende <- c("cat 1","cat 2")}
      if(i > 2){legende <- c(legende,paste("cat ", i, sep = ""))}}
    legend(x=0.7, y=(0.20+(k-3)*0.05),col=2:(k+1),lty =1,legend=legende)
    title(main = "Non-parametric calibration plot")
    par(new=F)}
```

 





