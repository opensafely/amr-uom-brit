---
title: "AB types"
output: 
  html_document:
   toc: true
   toc_depth: 2
   toc_float:
     collapsed: false
     smooth_scorll: false
---

```{r setup, include=FALSE}
library(knitr)
library(dplyr)

library('tidyverse')

library("ggplot2")

library(caret)
knitr::opts_chunk$set(echo = TRUE)
options(knitr.table.format="html")
```

```{r}
#df <- read_rds("/Users/yayang/Documents/GitHub/amr-uom-brit/output/matched_ab.rds")
df <- read_rds(here::here("output","matched_ab.rds"))
count(df)

df=df[!is.na(df$case),]
count(df)

df=df%>% select( "case",
                 "AB_1_type","AB_6wk_type" , "ab_6w_binary" ,
             "AB_6wk" ,"total_ab", "ab_prescriptions","ab_types",
              "prescribe_times","exposure_period"  ,
               "recent_ab_days",  
              "broad_prop" ,"broad_ab_prescriptions",
             "interval_med" ,  "interval_mean" , "interval_sd" , "interval_CV",
              "length_med" , "length_mean", "length_sd" ,  "length_CV" 
               )
df$exposure_period=-df$exposure_period
df[is.na(df)] <- 0
```


```{r eval=FALSE, include=FALSE}
# divide data
set.seed(1024)
all_idx <- 1:nrow(df)

train_idx <- sample(all_idx, nrow(df) * 0.8)
test_idx <- all_idx[!all_idx %in% train_idx]

df.train=df[train_idx,]
df.test=df[test_idx,]

length(all_idx)
length(train_idx)
length(test_idx)
```


```{r echo=TRUE}
# develop model
df$case=as.factor(df$case)

set.seed(123)
df=sample_n(df,1000)

x=df[,2:21]
y=df[,1]

```

# default
```{r }

#10 folds repeat 3 times
control <- trainControl(method='repeatedcv', 
                        number=5, 
                        repeats=3)
#Metric compare model is Accuracy
metric <- "Accuracy"
set.seed(123)
#Number randomely variable selected is mtry
mtry <- sqrt(ncol(x))
tunegrid <- expand.grid(.mtry=mtry)
rf_default <- train(case~., 
                      data=df, 
                      method='rf', 
                      metric='Accuracy', 
                      tuneGrid=tunegrid, 
                      trControl=control)
print(rf_default)

plot(varImp(rf_default,scale = F), main= "Var Imp: RF 5 fold CV")
```

# Random search
```{r }
#mtry: Number of random variables collected at each split. In normal equal square number columns.
mtry <- sqrt(ncol(x))
#ntree: Number of trees to grow.
ntree <- 3


control <- trainControl(method='repeatedcv', 
                        number=5, 
                        repeats=3,
                        search = 'random')

#Random generate 15 mtry values with tuneLength = 15
set.seed(1)
rf_random <- train(case ~ .,
                   data = df,
                   method = 'rf',
                   metric = 'Accuracy',
                   tuneLength  = 50, 
                   trControl = control)
print(rf_random)
plot(rf_random)
plot(varImp(rf_random,scale = F), main= "Var Imp: RF 5 fold CV")

```


# Grid search
```{r}

#Create control function for training with 10 folds and keep 3 folds for training. search method is grid.
control <- trainControl(method='repeatedcv', 
                        number=5, 
                        repeats=3, 
                        search='grid')
#create tunegrid with 15 values from 1:15 for mtry to tunning model. Our train function will change number of entry variable at each split according to tunegrid. 
tunegrid <- expand.grid(.mtry = c(1:20)) 

rf_gridsearch <- train(case ~ ., 
                       data = df,
                       method = 'rf',
                       metric = 'Accuracy',
                       tuneGrid = tunegrid,
                       trControl=control
                  )
print(rf_gridsearch)
plot(rf_gridsearch)
plot(varImp(rf_gridsearch,scale = F), main= "Var Imp: RF 5 fold CV")

```

# GBM
```{r eval=FALSE, include=FALSE}
fitControl <- trainControl(method = "repeatedcv",
                           number = 5,
                           repeats = 3,
                           ## Estimate class probabilities
                           classProbs = TRUE,
                           ## Evaluate performance using 
                           ## the following function
                           summaryFunction = twoClassSummary)
gbmGrid <-  expand.grid(interaction.depth = c(1, 5, 9), 
                        n.trees = (1:30)*50, 
                        shrinkage = 0.1,
                        n.minobsinnode = 20)
                        
nrow(gbmGrid)

set.seed(825)
gbmFit2 <- train(case ~ ., data =df, 
                 method = "gbm", 
                 trControl = fitControl, 
                 verbose = FALSE, 
                 ## Now specify the exact models 
                 ## to evaluate:
                 tuneGrid = gbmGrid)
gbmFit2
```


