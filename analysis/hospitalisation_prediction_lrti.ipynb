{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predcition of Hospital Admission Related to Lower Respiratory Tract Infection\n",
    "\n",
    "In this notebook, we develop Cox proportional hazard regression models to predict the risk of hospital admission related to lower respiratory tract infection (lrti)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines.utils import concordance_index\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from statsmodels.formula.api import logit\n",
    "from datetime import date\n",
    "from operator import attrgetter\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.warn('DelftStack')\n",
    "warnings.warn('Do not show this message')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0- functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to transform cph model summary and save it as html\n",
    "def GetPrintSummary(model):\n",
    "    output = \"\"\n",
    "    with io.StringIO() as buf, redirect_stdout(buf):\n",
    "        model.print_summary(style=\"html\")\n",
    "        output = buf.getvalue()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#round counts in a way that the last digit become 0 or 5\n",
    "def round_five_mul(x, base=5):\n",
    "    return base * round(x/base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to process infection data \n",
    "def proc_infec_data(data, i):\n",
    "    datum = data\n",
    "    datum = datum[['age', 'age_cat', 'sex', 'flu_vaccine', 'smoking', 'bmi', 'imd', 'ethnicity', \n",
    "                   'region', 'CCI', 'died_date', 'deregistered_date', 'practice', \n",
    "                   'antibacterial_brit_'+str(i), 'lrti_date_'+str(i), 'lrti_ab_date_'+str(i), 'lrti_ab_type_'+str(i),\n",
    "                   'incdt_lrti_date_'+str(i), 'admitted_lrti_date_'+str(i), 'sgss_gp_cov_lrti_date_'+str(i)]]\n",
    "    \n",
    "    #drop rows with no lrti reord\n",
    "    datum = datum[datum['lrti_date_'+str(i)].notnull()]\n",
    "    #exclusion of covid positive 90 days before and 30 days after dx with lrti_i\n",
    "    datum = datum[datum['sgss_gp_cov_lrti_date_'+str(i)] == 0]\n",
    "    #rename variables with i\n",
    "    datum.rename(columns={'lrti_date_'+str(i): 'lrti_date', 'lrti_ab_date_'+str(i): 'lrti_ab_date', \n",
    "                           'lrti_ab_type_'+str(i): 'ab_type', 'antibacterial_brit_'+str(i): 'antibacterial_brit',\n",
    "                           'incdt_lrti_date_'+str(i): 'incdt_lrti_date', 'incdt_lrti_type_'+str(i): 'incdt_lrti_type',\n",
    "                           'admitted_lrti_date_'+str(i): 'admitted_lrti_date',\n",
    "                           'sgss_gp_cov_lrti_date_'+str(i): 'sgss_gp_cov_lrti_date'},\n",
    "                inplace=True)\n",
    "    \n",
    "    return datum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparation of data for cox models\n",
    "def cox_data_prep(data, vars, vars_countinuous, vars_output):\n",
    "    data_req = data[vars+vars_countinuous+vars_output]\n",
    "    #encoding categorical data for cox models\n",
    "    data_enc = data_req[vars+vars_countinuous].reset_index()\n",
    "    #creating instance of one-hot-encoder\n",
    "    enc = OneHotEncoder()\n",
    "    data_req_enc = pd.DataFrame(enc.fit_transform(data_enc[vars]).toarray())\n",
    "    data_req_enc.columns = enc.get_feature_names(data_enc[vars].columns)\n",
    "    data_req_enc = data_enc[vars_countinuous].join(data_req_enc) \n",
    "\n",
    "    return data_req, data_req_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary table of observation counts\n",
    "def make_summary_table(infection, infection_type, data):\n",
    "    summary_table = data.sum(axis=0).reset_index()\n",
    "    summary_table.columns = ['variable', 'count']\n",
    "    #round count to make last digit either 0 or 5\n",
    "    summary_table['mean'] = data.mean(axis=0).reset_index()[0]\n",
    "    summary_table['count'] = round_five_mul(summary_table['count'], base=5)\n",
    "    summary_table['std'] = data.std(axis=0).reset_index()[0]\n",
    "    #replace small counts (<=5) with 'SM'\n",
    "    summary_table.loc[(summary_table['count'] <= 5), 'count'] = 'SM'\n",
    "\n",
    "    # save table\n",
    "    summary_table.to_csv('../output/hospitalisation_prediction_'+infection+'/summary_table_'+infection+'_'+infection_type+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary table of hospitalised cases\n",
    "def make_summary_table_hosped(infection, infection_type, data):\n",
    "    #keep hospitalised cses (events=1)\n",
    "    data_hosped = data[data['event_'+infection+'_admitted'] == 1]\n",
    "\n",
    "    #encoding categorical hosped data\n",
    "    data_hosped = data_hosped[['age_cat', 'sex', 'CCI_cat', 'flu_vaccine', 'bmi_cat', 'region', 'imd', 'ethnicity', 'smoking', 'season', 'period', 'antibacterial_brit']].reset_index()\n",
    "\n",
    "    #creating instance of one-hot-encoder\n",
    "    enc = OneHotEncoder()\n",
    "    data_hosped_enc = pd.DataFrame(enc.fit_transform(data_hosped[['age_cat', 'sex', 'CCI_cat', 'flu_vaccine', 'bmi_cat', 'region', 'imd', 'ethnicity', 'smoking', 'season', 'period']]).toarray())\n",
    "    data_hosped_enc.columns = enc.get_feature_names(data_hosped[['age_cat', 'sex', 'CCI_cat', 'flu_vaccine', 'bmi_cat', 'region', 'imd', 'ethnicity', 'smoking', 'season', 'period']].columns)\n",
    "    data_hosped_enc = data_hosped[['antibacterial_brit']].join(data_hosped_enc)\n",
    "\n",
    "    #summary table for hosped cases\n",
    "    summary_table_hosped = data_hosped_enc.sum(axis=0).reset_index()\n",
    "    summary_table_hosped.columns = ['variable', 'count']\n",
    "    summary_table_hosped['mean'] = data_hosped_enc.mean(axis=0).reset_index()[0]\n",
    "    #round count to make last digit either 0 or 5\n",
    "    summary_table_hosped['count'] = round_five_mul(summary_table_hosped['count'], base=5)\n",
    "    summary_table_hosped['std'] = data_hosped_enc.std(axis=0).reset_index()[0]\n",
    "    #replace small counts (<=5) with 'SM'\n",
    "    summary_table_hosped.loc[(summary_table_hosped['count'] <= 5), 'count'] = 'SM'\n",
    "\n",
    "    #save table\n",
    "    summary_table_hosped.to_csv('../output/hospitalisation_prediction_'+infection+'/summary_table_'+infection+'_'+infection_type+'_hosped.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build cox models\n",
    "def cox_model_build(infection, infection_type, data): \n",
    "    #randomly splitting data into training (%75) and testing (%25)\n",
    "    data_train, data_test = train_test_split(data, test_size=0.25, random_state=42)\n",
    "\n",
    "    cph = CoxPHFitter()\n",
    "    cph.fit(data_train,\n",
    "            duration_col='duration_'+infection+'_admitted',\n",
    "            event_col='event_'+infection+'_admitted',\n",
    "            formula=\"age_cat + sex + CCI_cat + flu_vaccine + region + imd + ethnicity + smoking + season + bmi_cat + antibacterial_brit\")\n",
    "    cph.print_summary()\n",
    "\n",
    "    #save model output\n",
    "    output = GetPrintSummary(cph)\n",
    "    with open('../output/hospitalisation_prediction_'+infection+'/cph_'+infection+'_'+infection_type+'_summary.html', \"w\") as fo:\n",
    "        print(fo.write(output))\n",
    "        fo.close()\n",
    "\n",
    "    #calculate and save ci\n",
    "    ci = cph.concordance_index_\n",
    "    a = open('../output/hospitalisation_prediction_'+infection+'/'+infection+'_'+infection_type+'_ci.txt', \"w\")\n",
    "    a.write(\"\")\n",
    "    a.writelines(['Concordance index for '+infection+'_'+infection_type+' with development data: %.4f' % (ci)])\n",
    "\n",
    "    return cph, data_train, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save coefficients (betas) and calculate sum of betas\n",
    "def cox_betas_sumbetas_calculate(cph, directory, betas_name, data_infection_enc, data_infection, sum_betas_name):\n",
    "    # cleaning up covariates to make them compatible with the encoded data\n",
    "    covariates = cph.params_.reset_index()#.hazard_ratios_\n",
    "    covariates.covariate = covariates.covariate.str.replace(\"[\",\"\")\n",
    "    covariates.covariate = covariates.covariate.str.replace(\"]\",\"\")\n",
    "    covariates.covariate = covariates.covariate.str.replace(\"T.\",\"_\")\n",
    "    #save coefficients\n",
    "    covariates.to_csv(directory+betas_name, index=False)\n",
    "\n",
    "    #save hazard ratios\n",
    "    covariates_betas = cph.hazard_ratios_.reset_index()\n",
    "    confidence_intervals = cph.confidence_intervals_.reset_index()\n",
    "    hrs = covariates_betas.merge(confidence_intervals, on='covariate', how='left')\n",
    "    hrs['95% lower-bound'] = np.exp(hrs['95% lower-bound'])\n",
    "    hrs['95% upper-bound'] = np.exp(hrs['95% upper-bound'])\n",
    "    hrs.covariate = hrs.covariate.str.replace(\"[\",\"\")\n",
    "    hrs.covariate = hrs.covariate.str.replace(\"]\",\"\")\n",
    "    hrs.covariate = hrs.covariate.str.replace(\"T.\",\"_\")\n",
    "    hrs.to_csv(directory+'hazard_ratios_'+betas_name, index=False)\n",
    "\n",
    "    #transpose and sort covariates and coefficients\n",
    "    covariatesT = covariates.T\n",
    "    header = covariatesT.iloc[0]\n",
    "    covariatesT = covariatesT[1:]\n",
    "    covariatesT.columns = header\n",
    "    covariatesT = covariatesT.sort_index(axis = 1) #sort covariates to match data\n",
    "\n",
    "    #further cleaning up covariates to make them compatible with the encoded data\n",
    "    covariates_list = [\"'\" + item + \"'\" for item in covariates.covariate] #adds single quotation mark\n",
    "    covariates_list = [value.replace(\"'\", \"\") for value in covariates_list] #removes double quotation mark \n",
    "\n",
    "    #match covariates_list (covariates of cox model) with encoded data\n",
    "    data = data_infection_enc[data_infection_enc.columns.intersection(covariates_list)] \n",
    "    data = data.sort_index(axis = 1) #sort \n",
    "\n",
    "    #multiply betas and encoded data\n",
    "    betas = data.multiply(np.array(covariatesT), axis='columns')\n",
    "\n",
    "    #sum up betas of each row to calculate sum of betas \n",
    "    sum_betas = betas.sum(axis=1)\n",
    "    \n",
    "    #predict cumulative hazard values at day 30 and match them with sum of betas\n",
    "    sum_betas_hazard = cph.predict_cumulative_hazard(data_infection).loc[[30]].T.reset_index(drop=True)\n",
    "    sum_betas_hazard['sum_betas'] = sum_betas\n",
    "\n",
    "    #drop rows with predicted risks greater than 99th percentile of all risks\n",
    "    sum_betas_hazard = sum_betas_hazard[sum_betas_hazard[30] <= sum_betas_hazard[30].quantile(0.99)]  \n",
    "\n",
    "    #sort values of sum of betas\n",
    "    sum_betas_hazard = sum_betas_hazard.sort_values(by=['sum_betas'])\n",
    "\n",
    "    # binning sum of betas into 100 bins\n",
    "    sum_betas_hazard['bins'] = pd.cut(sum_betas_hazard['sum_betas'], 100) #100 intervals between min and max of sum_betas\n",
    "\n",
    "    #calculate mean of cumulative hazar at day 30 by groupby \n",
    "    sum_betas_hazard_groupby = sum_betas_hazard.groupby(['bins'])[30.0].describe().reset_index()\n",
    "\n",
    "    #calculate interpolation of missing mean values \n",
    "    sum_betas_hazard_groupby['mean_interpolate'] = sum_betas_hazard_groupby['mean'].interpolate(method='linear')\n",
    "\n",
    "    #separate upper and lower boundries of bins\n",
    "    sum_betas_hazard_groupby['bin_lower'] = sum_betas_hazard_groupby['bins'].map(attrgetter('left'))\n",
    "    sum_betas_hazard_groupby['bin_upper'] = sum_betas_hazard_groupby['bins'].map(attrgetter('right'))\n",
    "\n",
    "    #select relevant columns including mean of cumulative hazard for each bin and the interpolation of means of cumulative hazard\n",
    "    sum_betas = sum_betas_hazard_groupby[['bin_lower', 'bin_upper', 'mean_interpolate']]\n",
    "\n",
    "    #save relevant columns of sum_betas\n",
    "    sum_betas.to_csv(directory+sum_betas_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cox_roc_curve(cph, event_col_name, data_train, data_test, directory, roc_name):\n",
    "    #analysis of discrimination (training) by calculating cumulative hazard at day 30 and comparing it with variable event  \n",
    "    cph_train_cumulative_hazard = cph.predict_cumulative_hazard(data_train)\n",
    "    cph_train_cumulative_hazard = cph_train_cumulative_hazard.loc[[30]].T\n",
    "    cph_train_cumulative_hazard[event_col_name] = data_train[event_col_name]\n",
    "    cph_train_cumulative_hazard = cph_train_cumulative_hazard.rename(columns={30: 'cumulative_hazard'}, inplace=False)\n",
    "\n",
    "    #analysis of discrimination (testing) by calculating cumulative hazard at day 30 and comparing it with variable event  \n",
    "    cph_test_cumulative_hazard = cph.predict_cumulative_hazard(data_test)\n",
    "    cph_test_cumulative_hazard = cph_test_cumulative_hazard.loc[[30]].T\n",
    "    cph_test_cumulative_hazard[event_col_name] = data_test[event_col_name]\n",
    "    cph_test_cumulative_hazard = cph_test_cumulative_hazard.rename(columns={30: 'cumulative_hazard'}, inplace=False)\n",
    "\n",
    "    #plot roc curves\n",
    "    r_fpr1, r_tpr1, _ = roc_curve(cph_train_cumulative_hazard[event_col_name], cph_train_cumulative_hazard.cumulative_hazard)\n",
    "    r_fpr2, r_tpr2, _ = roc_curve(cph_test_cumulative_hazard[event_col_name], cph_test_cumulative_hazard.cumulative_hazard)\n",
    "\n",
    "    fig, ax1 = pyplot.subplots(figsize=(7, 7))\n",
    "    line1, = plt.plot(r_fpr1, r_tpr1, linestyle='-', marker='o', markersize=6, markevery=0.1, color='black', label='Cox with development data')\n",
    "    line2, = plt.plot(r_fpr2, r_tpr2, linestyle='--', marker='v', markersize=6, markevery=0.1, color='red', label='Cox with validation data')\n",
    "\n",
    "    squares = [0,1.01]\n",
    "    plt.plot(squares,linewidth=1, color='grey')\n",
    "    plt.ylim(0,1.01)\n",
    "    plt.xlim(0,1)\n",
    "    plt.xlabel('Specificity', fontsize=14)\n",
    "    plt.ylabel('Sensitivity', fontsize=14)\n",
    "\n",
    "    #reversing xticks\n",
    "    xticks = [1.0, 0.8, 0.6, 0.4, 0.2, 0.0]\n",
    "    x = np.arange(len(xticks))\n",
    "    ax1.set(xticklabels=xticks)\n",
    "    ax1.legend(fontsize=14)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12, rotation=0)\n",
    "    plt.savefig(directory+'/'+roc_name, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate and plot calibrations of cox models\n",
    "def cox_calibration(cph, data_train, event_name, data_test, directory, calib_name):\n",
    "    #predict cumulative hazard in day 30\n",
    "    prob_pred_true_train = cph.predict_cumulative_hazard(data_train).loc[[30]].T\n",
    "    prob_pred_true_train.columns = ['pred']\n",
    "\n",
    "    #group cumulative hazard into 10 bins with equal frequency of observations in each bin\n",
    "    prob_pred_true_train['bins'] = pd.qcut(prob_pred_true_train['pred'], 10)\n",
    "\n",
    "    #merge cumulative hazards with their actual events (0, 1)\n",
    "    prob_pred_true_train = pd.merge(prob_pred_true_train, data_train[event_name], left_index=True, right_index=True)\n",
    "\n",
    "    #groupby bins to find mean predicted probability for each bin (pred_mean), count of events in each bin (event_sum) and count of samples in each bin (event_count)\n",
    "    prob_pred_true_train_groupby_bin = prob_pred_true_train.groupby('bins')[['pred', event_name]].agg(['mean', 'sum', 'count']).reset_index()\n",
    "    prob_pred_true_train_groupby_bin.columns = ['bins', 'pred_mean', 'pred_sum', 'pred_count', 'event_mean', 'event_sum', 'event_count']\n",
    "\n",
    "    #calculate proportion of events in each bin\n",
    "    prob_pred_true_train_groupby_bin['event_proportion'] = prob_pred_true_train_groupby_bin['event_sum']/prob_pred_true_train_groupby_bin['event_count']\n",
    "\n",
    "    #predict cumulative hazard in day 30\n",
    "    prob_pred_true_test = cph.predict_cumulative_hazard(data_test).loc[[30]].T \n",
    "    prob_pred_true_test.columns = ['pred']\n",
    "\n",
    "    #group cumulative hazard into 10 bins with equal frequency of observations in each bin\n",
    "    prob_pred_true_test['bins'] = pd.qcut(prob_pred_true_test['pred'], 10)\n",
    "\n",
    "    #merge cumulative hazards with their actual events (0, 1)\n",
    "    prob_pred_true_test = pd.merge(prob_pred_true_test, data_test[event_name], left_index=True, right_index=True)\n",
    "\n",
    "    #groupby bins to find mean predicted probability for each bin (pred_mean), count of events in each bin (event_sum) and count of samples in each bin (event_count)\n",
    "    prob_pred_true_test_groupby_bin = prob_pred_true_test.groupby('bins')[['pred', event_name]].agg(['mean', 'sum', 'count']).reset_index()\n",
    "    prob_pred_true_test_groupby_bin.columns = ['bins', 'pred_mean', 'pred_sum', 'pred_count', 'event_mean', 'event_sum', 'event_count']\n",
    "\n",
    "    #calculate proportion of events in each bin\n",
    "    prob_pred_true_test_groupby_bin['event_proportion'] = prob_pred_true_test_groupby_bin['event_sum']/prob_pred_true_test_groupby_bin['event_count']\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(7, 7))\n",
    "    plt.plot(prob_pred_true_train_groupby_bin.pred_mean, prob_pred_true_train_groupby_bin.event_proportion, color='#6ce0c9', linestyle='solid', marker='o', alpha=0.9)#, marker='o', facecolors='none', edgecolors='g', alpha=0.6)\n",
    "    plt.plot(prob_pred_true_test_groupby_bin.pred_mean, prob_pred_true_test_groupby_bin.event_proportion, color='#eb91b7', linestyle='dashed', marker='v', alpha=0.9)\n",
    "    plt.xlabel('Mean predicted probabilities', fontsize=14)\n",
    "    plt.ylabel('Proportion of observed values', fontsize=14)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12, rotation=0)\n",
    "    plt.xlim(0, max(prob_pred_true_train_groupby_bin.pred_mean.max(), prob_pred_true_train_groupby_bin.event_proportion.max(), max(prob_pred_true_test_groupby_bin.pred_mean.max(), prob_pred_true_test_groupby_bin.event_proportion.max()).round(decimals = 2) + (max(prob_pred_true_train_groupby_bin.pred_mean.max(), prob_pred_true_train_groupby_bin.event_proportion.max(), max(prob_pred_true_test_groupby_bin.pred_mean.max(), prob_pred_true_test_groupby_bin.event_proportion.max())).round(decimals = 2)/3)))\n",
    "    plt.ylim(0, max(prob_pred_true_train_groupby_bin.pred_mean.max(), prob_pred_true_train_groupby_bin.event_proportion.max(), max(prob_pred_true_test_groupby_bin.pred_mean.max(), prob_pred_true_test_groupby_bin.event_proportion.max()).round(decimals = 2) + (max(prob_pred_true_train_groupby_bin.pred_mean.max(), prob_pred_true_train_groupby_bin.event_proportion.max(), max(prob_pred_true_test_groupby_bin.pred_mean.max(), prob_pred_true_test_groupby_bin.event_proportion.max())).round(decimals = 2)/3)))\n",
    "    plt.plot([0, 1], [0, 1], linewidth=1, linestyle='-', color='grey')\n",
    "    legend_dev = mlines.Line2D([], [], color='#6ce0c9', linestyle='-', marker='o', markersize=10, label='Development data', alpha=.9)\n",
    "    legend_val = mlines.Line2D([], [], color='#eb91b7', linestyle='--', marker='v', markersize=10, label='Validation data', alpha=.9)\n",
    "    plt.legend(handles=[legend_dev, legend_val])#, title=\"infection\")\n",
    "    plt.savefig(directory+calib_name, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print outputs of cox models\n",
    "def cox_output(cph, infection, infection_type, data_train, data_test, event_name, duration_name, directory):\n",
    "    #calculate concordance index\n",
    "    concord_index_train = concordance_index(data_train[duration_name], -cph.predict_partial_hazard(data_train), data_train[event_name])\n",
    "    concord_index_test = concordance_index(data_test[duration_name], -cph.predict_partial_hazard(data_test), data_test[event_name])\n",
    "    \n",
    "    #save model outputs\n",
    "    a = open(directory+'cph_'+infection+'_'+infection_type+'_output.txt', \"w\")\n",
    "    a.write(\"\")\n",
    "    a.writelines(['Concordance index for '+infection+'_'+infection_type+' with development data: %.4f' % (concord_index_train), \n",
    "                '\\nConcordance index for '+infection+'_'+infection_type+' with validation data: %.4f' % (concord_index_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build cox model with an additional binary variable for antibiotics\n",
    "def cox_model_abs_build(infection, infection_type, data):\n",
    "    #randomly splitting data into training (%75) and testing (%25)\n",
    "    data_train, data_test = train_test_split(data, test_size=0.25, random_state=42)\n",
    "\n",
    "    cph = CoxPHFitter()\n",
    "    cph.fit(data_train,\n",
    "            duration_col='duration_'+infection+'_admitted',\n",
    "            event_col='event_'+infection+'_admitted',\n",
    "            formula=\"age_cat + sex + CCI_cat + flu_vaccine + region + imd + ethnicity + smoking + season + bmi_cat + antibacterial_brit + lrti_ab_date\")\n",
    "    cph.print_summary()\n",
    "\n",
    "    #save model output\n",
    "    output = GetPrintSummary(cph)\n",
    "    with open('../output/hospitalisation_prediction_'+infection+'/cph_'+infection+'_'+infection_type+'_summary.html', \"w\") as fo:\n",
    "        print(fo.write(output))\n",
    "        fo.close()\n",
    "\n",
    "    #save hazard ratios\n",
    "    covariates_betas = cph.hazard_ratios_.reset_index()\n",
    "    confidence_intervals = cph.confidence_intervals_.reset_index()\n",
    "    hrs = covariates_betas.merge(confidence_intervals, on='covariate', how='left')\n",
    "    hrs['95% lower-bound'] = np.exp(hrs['95% lower-bound'])\n",
    "    hrs['95% upper-bound'] = np.exp(hrs['95% upper-bound'])\n",
    "    hrs.covariate = hrs.covariate.str.replace(\"[\",\"\")\n",
    "    hrs.covariate = hrs.covariate.str.replace(\"]\",\"\")\n",
    "    hrs.covariate = hrs.covariate.str.replace(\"T.\",\"_\")\n",
    "    hrs.to_csv('../output/hospitalisation_prediction_'+infection+'/cph_'+infection+'_'+infection_type+'_hrs.csv', index=False)\n",
    "\n",
    "    ##add deciles of predicted risks and probability of prescribed antibiotics \n",
    "    #predict cumulative hazard in day 30 in train dataset\n",
    "    prob_pred_train = cph.predict_cumulative_hazard(data_train).loc[[30]].T\n",
    "    prob_pred_train.columns = ['pred']\n",
    "    #group cumulative hazard into 10 bins with equal frequency of observations in each bin\n",
    "    prob_pred_train['bins'] = pd.qcut(prob_pred_train['pred'], 10)\n",
    "    #add a column for prescribed antibiotics\n",
    "    prob_pred_train = pd.merge(prob_pred_train, data_train[infection+'_ab_date'], left_index=True, right_index=True)\n",
    "    #groupby bins to find mean predicted probability for each bin (pred_mean) and mean probability of being prescribed antibiotics (ab_prob_mean)\n",
    "    prob_pred_groupby_bin_train = prob_pred_train.groupby('bins')[['pred', infection+'_ab_date']].agg(['mean', 'sum', 'count']).reset_index()\n",
    "    prob_pred_groupby_bin_train.columns = ['bins', 'pred_mean_train', 'pred_sum', 'pred_count', 'ab_prob_mean_train', 'ab_prob_sum', 'ab_prob_count']\n",
    "    prob_pred_groupby_bin = prob_pred_groupby_bin_train[['pred_mean_train', 'ab_prob_mean_train']]\n",
    "\n",
    "    #predict cumulative hazard in day 30 in test dataset\n",
    "    prob_pred_test = cph.predict_cumulative_hazard(data_test).loc[[30]].T\n",
    "    prob_pred_test.columns = ['pred']\n",
    "    #group cumulative hazard into 10 bins with equal frequency of observations in each bin\n",
    "    prob_pred_test['bins'] = pd.qcut(prob_pred_test['pred'], 10)\n",
    "    #add a column for prescribed antibiotics\n",
    "    prob_pred_test = pd.merge(prob_pred_test, data_test[infection+'_ab_date'], left_index=True, right_index=True)\n",
    "    #groupby bins to find mean predicted probability for each bin (pred_mean) and mean probability of being prescribed antibiotics (ab_prob_mean)\n",
    "    prob_pred_groupby_bin_test = prob_pred_test.groupby('bins')[['pred', infection+'_ab_date']].agg(['mean', 'sum', 'count']).reset_index()\n",
    "    prob_pred_groupby_bin_test.columns = ['bins', 'pred_mean_test', 'pred_sum', 'pred_count', 'ab_prob_mean_test', 'ab_prob_sum', 'ab_prob_count']\n",
    "    prob_pred_groupby_bin[['pred_mean_test', 'ab_prob_mean_test']] = prob_pred_groupby_bin_test[['pred_mean_test', 'ab_prob_mean_test']]\n",
    "\n",
    "    #save binned prob_pred, train and test combined\n",
    "    prob_pred_groupby_bin.to_csv('../output/hospitalisation_prediction_'+infection+'/prob_pred_groupby_bin_'+infection+'_'+infection_type+'.csv', index=False)\n",
    "\n",
    "    return cph, data_train, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build cox model with an additional categorical variable for antibiotics\n",
    "def cox_model_ab_type_build(infection, infection_type, data): \n",
    "    #randomly splitting data into training (%75) and testing (%25)\n",
    "    data_train, data_test = train_test_split(data, test_size=0.25, random_state=42)\n",
    "\n",
    "    cph = CoxPHFitter()\n",
    "    cph.fit(data_train,\n",
    "            duration_col='duration_'+infection+'_admitted',\n",
    "            event_col='event_'+infection+'_admitted',\n",
    "            formula=\"age_cat + sex + CCI_cat + flu_vaccine + region + imd + ethnicity + smoking + season + bmi_cat + antibacterial_brit + ab_type_cat\")\n",
    "    cph.print_summary()\n",
    "\n",
    "    #save model output\n",
    "    output = GetPrintSummary(cph)\n",
    "    with open('../output/hospitalisation_prediction_'+infection+'/cph_'+infection+'_'+infection_type+'_summary.html', \"w\") as fo:\n",
    "        print(fo.write(output))\n",
    "        fo.close()\n",
    "\n",
    "    #save hazard ratios\n",
    "    covariates_betas = cph.hazard_ratios_.reset_index()\n",
    "    confidence_intervals = cph.confidence_intervals_.reset_index()\n",
    "    hrs = covariates_betas.merge(confidence_intervals, on='covariate', how='left')\n",
    "    hrs['95% lower-bound'] = np.exp(hrs['95% lower-bound'])\n",
    "    hrs['95% upper-bound'] = np.exp(hrs['95% upper-bound'])\n",
    "    hrs.covariate = hrs.covariate.str.replace(\"[\",\"\")\n",
    "    hrs.covariate = hrs.covariate.str.replace(\"]\",\"\")\n",
    "    hrs.covariate = hrs.covariate.str.replace(\"T.\",\"_\")\n",
    "    hrs.to_csv('../output/hospitalisation_prediction_'+infection+'/cph_'+infection+'_'+infection_type+'_hrs.csv', index=False)\n",
    "\n",
    "    return cph, data_train, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build cox models stratified with sex categories \n",
    "def cox_model_strat_sex_build(infection, infection_type, data): \n",
    "    #randomly splitting data into training (%75) and testing (%25)\n",
    "    data_train, data_test = train_test_split(data, test_size=0.25, random_state=42)\n",
    "\n",
    "    cph = CoxPHFitter()\n",
    "    cph.fit(data_train,\n",
    "            duration_col='duration_'+infection+'_admitted',\n",
    "            event_col='event_'+infection+'_admitted',\n",
    "            formula=\"age_cat + CCI_cat + flu_vaccine + region + imd + ethnicity + smoking + season + bmi_cat + antibacterial_brit + lrti_ab_date\")\n",
    "    cph.print_summary()\n",
    "\n",
    "    #save model output\n",
    "    output = GetPrintSummary(cph)\n",
    "    with open('../output/hospitalisation_prediction_'+infection+'/cph_'+infection+'_'+infection_type+'_summary.html', \"w\") as fo:\n",
    "        print(fo.write(output))\n",
    "        fo.close()\n",
    "\n",
    "    #save hazard ratios\n",
    "    covariates_betas = cph.hazard_ratios_.reset_index()\n",
    "    confidence_intervals = cph.confidence_intervals_.reset_index()\n",
    "    hrs = covariates_betas.merge(confidence_intervals, on='covariate', how='left')\n",
    "    hrs['95% lower-bound'] = np.exp(hrs['95% lower-bound'])\n",
    "    hrs['95% upper-bound'] = np.exp(hrs['95% upper-bound'])\n",
    "    hrs.covariate = hrs.covariate.str.replace(\"[\",\"\")\n",
    "    hrs.covariate = hrs.covariate.str.replace(\"]\",\"\")\n",
    "    hrs.covariate = hrs.covariate.str.replace(\"T.\",\"_\")\n",
    "    hrs.to_csv('../output/hospitalisation_prediction_'+infection+'/cph_'+infection+'_'+infection_type+'_hrs.csv', index=False)\n",
    "\n",
    "    return cph, data_train, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build cox models stratified with age categories\n",
    "def cox_model_strat_age_build(infection, infection_type, data): \n",
    "    #randomly splitting data into training (%75) and testing (%25)\n",
    "    data_train, data_test = train_test_split(data, test_size=0.25, random_state=42)\n",
    "\n",
    "    cph = CoxPHFitter()\n",
    "    cph.fit(data_train,\n",
    "            duration_col='duration_'+infection+'_admitted',\n",
    "            event_col='event_'+infection+'_admitted',\n",
    "            formula=\"sex + CCI_cat + flu_vaccine + region + imd + ethnicity + smoking + season + bmi_cat + antibacterial_brit + lrti_ab_date\")\n",
    "    cph.print_summary()\n",
    "\n",
    "    #save model output\n",
    "    output = GetPrintSummary(cph)\n",
    "    with open('../output/hospitalisation_prediction_'+infection+'/cph_'+infection+'_'+infection_type+'_summary.html', \"w\") as fo:\n",
    "        print(fo.write(output))\n",
    "        fo.close()\n",
    "\n",
    "    #save hazard ratios\n",
    "    covariates_betas = cph.hazard_ratios_.reset_index()\n",
    "    confidence_intervals = cph.confidence_intervals_.reset_index()\n",
    "    hrs = covariates_betas.merge(confidence_intervals, on='covariate', how='left')\n",
    "    hrs['95% lower-bound'] = np.exp(hrs['95% lower-bound'])\n",
    "    hrs['95% upper-bound'] = np.exp(hrs['95% upper-bound'])\n",
    "    hrs.covariate = hrs.covariate.str.replace(\"[\",\"\")\n",
    "    hrs.covariate = hrs.covariate.str.replace(\"]\",\"\")\n",
    "    hrs.covariate = hrs.covariate.str.replace(\"T.\",\"_\")\n",
    "    hrs.to_csv('../output/hospitalisation_prediction_'+infection+'/cph_'+infection+'_'+infection_type+'_hrs.csv', index=False)\n",
    "\n",
    "    return cph, data_train, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build cox models stratified with time period categories \n",
    "def cox_model_strat_period_build(infection, infection_type, data):\n",
    "    #randomly splitting data into training (%75) and testing (%25)\n",
    "    data_train, data_test = train_test_split(data, test_size=0.25, random_state=42)\n",
    "\n",
    "    cph = CoxPHFitter()\n",
    "    cph.fit(data_train,\n",
    "            duration_col='duration_'+infection+'_admitted',\n",
    "            event_col='event_'+infection+'_admitted',\n",
    "            formula=\"age_cat + sex + CCI_cat + flu_vaccine + region + imd + ethnicity + smoking + season + bmi_cat + antibacterial_brit + lrti_ab_date\")\n",
    "    cph.print_summary()\n",
    "\n",
    "    #save model output\n",
    "    output = GetPrintSummary(cph)\n",
    "    with open('../output/hospitalisation_prediction_'+infection+'/cph_'+infection+'_'+infection_type+'_summary.html', \"w\") as fo:\n",
    "        print(fo.write(output))\n",
    "        fo.close()\n",
    "\n",
    "    #save hazard ratios\n",
    "    covariates_betas = cph.hazard_ratios_.reset_index()\n",
    "    confidence_intervals = cph.confidence_intervals_.reset_index()\n",
    "    hrs = covariates_betas.merge(confidence_intervals, on='covariate', how='left')\n",
    "    hrs['95% lower-bound'] = np.exp(hrs['95% lower-bound'])\n",
    "    hrs['95% upper-bound'] = np.exp(hrs['95% upper-bound'])\n",
    "    hrs.covariate = hrs.covariate.str.replace(\"[\",\"\")\n",
    "    hrs.covariate = hrs.covariate.str.replace(\"]\",\"\")\n",
    "    hrs.covariate = hrs.covariate.str.replace(\"T.\",\"_\")\n",
    "    hrs.to_csv('../output/hospitalisation_prediction_'+infection+'/cph_'+infection+'_'+infection_type+'_hrs.csv', index=False)\n",
    "\n",
    "    return cph, data_train, data_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data\n",
    "data = pd.read_csv(f'../output/hospitalisation_data/input_hospitalisation_lrti.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate Chalrson comorbidity index (CCI)\n",
    "data['CCI'] = np.nan\n",
    "\n",
    "for idx, x in enumerate(data['CCI']):\n",
    "    n=0\n",
    "    if data.iat[idx, data.columns.get_loc('mi_comor')] == 1: \n",
    "        n=n+1\n",
    "    if data.iat[idx, data.columns.get_loc('cardiovascular_comor')] == 1: \n",
    "        n=n+1 \n",
    "    if data.iat[idx, data.columns.get_loc('peripheral_vascular_comor')] == 1: \n",
    "        n=n+1 \n",
    "    if data.iat[idx, data.columns.get_loc('chronic_obstructive_pulmonary_comor')] == 1: \n",
    "        n=n+1\n",
    "    if data.iat[idx, data.columns.get_loc('diabetes_comor')] == 1: \n",
    "        n=n+1\n",
    "    if data.iat[idx, data.columns.get_loc('dementia_comor')] == 1: \n",
    "        n=n+1\n",
    "    if data.iat[idx, data.columns.get_loc('peptic_ulcer_comor')] == 1:\n",
    "        n=n+1\n",
    "    if data.iat[idx, data.columns.get_loc('connective_tissue_comor')] == 1:\n",
    "        n=n+1\n",
    "    if data.iat[idx, data.columns.get_loc('mild_liver_comor')] == 1: \n",
    "        n=n+1\n",
    "    if data.iat[idx, data.columns.get_loc('heart_failure_comor')] == 1: \n",
    "        n=n+1\n",
    "    if data.iat[idx, data.columns.get_loc('hemiplegia_comor')] == 1: \n",
    "        n=n+2\n",
    "    if data.iat[idx, data.columns.get_loc('mod_severe_renal_comor')] == 1:\n",
    "        n=n+2\n",
    "    if data.iat[idx, data.columns.get_loc('diabetes_complications_comor')] == 1:\n",
    "        n=n+2\n",
    "    if data.iat[idx, data.columns.get_loc('cancer_comor')] == 1:\n",
    "        n=n+2\n",
    "    if data.iat[idx, data.columns.get_loc('mod_severe_liver_comor')] == 1:\n",
    "        n=n+3\n",
    "    if data.iat[idx, data.columns.get_loc('metastatic_cancer_comor')] == 1:\n",
    "        n=n+6\n",
    "    if data.iat[idx, data.columns.get_loc('hiv_comor')] == 1:\n",
    "        n=n+6\n",
    "    \n",
    "    data.iat[idx, data.columns.get_loc('CCI')]=n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- data preparation\n",
    "\n",
    "### 2-1- gathering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process 20 lrtis and concatenate them \n",
    "data_lrti_1 = proc_infec_data(data,1)\n",
    "data_lrti_2 = proc_infec_data(data,2)\n",
    "data_lrti_3 = proc_infec_data(data,3)\n",
    "data_lrti_4 = proc_infec_data(data,4)\n",
    "data_lrti_5 = proc_infec_data(data,5)\n",
    "data_lrti_6 = proc_infec_data(data,6)\n",
    "data_lrti_7 = proc_infec_data(data,7)\n",
    "data_lrti_8 = proc_infec_data(data,8)\n",
    "data_lrti_9 = proc_infec_data(data,9)\n",
    "data_lrti_10 = proc_infec_data(data,10)\n",
    "data_lrti_11 = proc_infec_data(data,11)\n",
    "data_lrti_12 = proc_infec_data(data,12)\n",
    "data_lrti_13 = proc_infec_data(data,13)\n",
    "data_lrti_14 = proc_infec_data(data,14)\n",
    "data_lrti_15 = proc_infec_data(data,15)\n",
    "data_lrti_16 = proc_infec_data(data,16)\n",
    "data_lrti_17 = proc_infec_data(data,17)\n",
    "data_lrti_18 = proc_infec_data(data,18)\n",
    "data_lrti_19 = proc_infec_data(data,19)\n",
    "data_lrti_20 = proc_infec_data(data,20)\n",
    "\n",
    "data_lrti = pd.concat([data_lrti_1, data_lrti_2, data_lrti_3, data_lrti_4, data_lrti_5, data_lrti_6, data_lrti_7, data_lrti_8, data_lrti_9, data_lrti_10, data_lrti_11, data_lrti_12, data_lrti_13, data_lrti_14, data_lrti_15, data_lrti_16, data_lrti_17, data_lrti_18, data_lrti_19, data_lrti_20])\n",
    "data_lrti.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2- add season, event, and duration variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "##add variable season\n",
    "#convert data types\n",
    "data_lrti['lrti_date'] = data_lrti['lrti_date'].astype('datetime64[ns]')\n",
    "data_lrti['admitted_lrti_date'] = data_lrti['admitted_lrti_date'].astype('datetime64[ns]')\n",
    "data_lrti.died_date = data_lrti.died_date.astype('datetime64[ns]')\n",
    "data_lrti.deregistered_date = data_lrti.deregistered_date.astype('datetime64[ns]')\n",
    "\n",
    "#add a variable called date using gp consultation dates\n",
    "data_lrti['date'] = data_lrti['lrti_date'] \n",
    "data_lrti['date'] = data_lrti['date'].dt.strftime('%Y-%m')\n",
    "\n",
    "#get today's date in year and month\n",
    "today_date_y_m = date.today()\n",
    "today_date_y_m = today_date_y_m.strftime('%Y-%m')\n",
    "\n",
    "#drop any records of data_lrti with today's date in year and month\n",
    "data_lrti = data_lrti[data_lrti['date'] != today_date_y_m]\n",
    "\n",
    "#get two months before today's date in year and month \n",
    "last_1_month_date_y_m = date.today() - pd.DateOffset(months=1)\n",
    "last_1_month_date_y_m = last_1_month_date_y_m.strftime('%Y-%m')\n",
    "last_2_month_date_y_m = date.today() - pd.DateOffset(months=2)\n",
    "last_2_month_date_y_m = last_2_month_date_y_m.strftime('%Y-%m')\n",
    "#drop any record of data with two month before today's date in year and month\n",
    "data_lrti = data_lrti[data_lrti['date'] != last_1_month_date_y_m]\n",
    "data_lrti = data_lrti[data_lrti['date'] != last_2_month_date_y_m]\n",
    "\n",
    "#add a variable called season based on the month of lrti records\n",
    "data_lrti['season'] = np.nan\n",
    "data_lrti['date_month'] = pd.DatetimeIndex(data_lrti['date']).month\n",
    "\n",
    "conditions = [\n",
    "    (data_lrti['date_month'] >= 3) & (data_lrti['date_month'] <= 5),\n",
    "    (data_lrti['date_month'] >= 6) & (data_lrti['date_month'] <= 8),\n",
    "    (data_lrti['date_month'] >= 9) & (data_lrti['date_month'] <= 11),]\n",
    "choices = ['spring', 'summer', 'autumn']\n",
    "data_lrti['season'] = np.select(conditions, choices, default='winter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add variable time period\n",
    "data_lrti.loc[(data_lrti['date'] <= '2019-12'), 'period'] = 'prepandemic'\n",
    "data_lrti.loc[((data_lrti['date'] >= '2020-01') & (data_lrti['date'] <= '2021-03')), 'period'] = 'during_pandemic'\n",
    "data_lrti.loc[(data_lrti['date'] >= '2021-04'), 'period'] = 'post_2nd_lockdown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-01'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lrti['date'].max()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### event and duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scenario 1\n",
    "#not hosped (nothing happened)\n",
    "data_lrti.loc[data_lrti['admitted_lrti_date'].isnull(), 'event_lrti_admitted'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scenario 2 \n",
    "#become a case (uncensoring)\n",
    "#calculating days between infection gp consultation and hosp\n",
    "data_lrti['delta_lrti_admitted'] = (data_lrti['admitted_lrti_date'] - data_lrti['lrti_date']).dt.days\n",
    "data_lrti.loc[((data_lrti['delta_lrti_admitted'] >= 0) & (data_lrti['delta_lrti_admitted'] <= 30)), 'event_lrti_admitted'] = 1\n",
    "\n",
    "#scenario 2\n",
    "#drop whoever was admitted before lrti consultation, i.e. negative value for delta_lrti_admitted\n",
    "data_lrti = data_lrti[~(data_lrti['delta_lrti_admitted'] < 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scenario 3\n",
    "#censor died patients\n",
    "data_lrti['delta_admitted_died'] = (data_lrti['died_date'] - data_lrti['admitted_lrti_date']).dt.days\n",
    "data_lrti.loc[data_lrti['delta_admitted_died'] < 0, 'delta_admitted_died'] = np.NaN\n",
    "data_lrti.loc[((data_lrti['delta_admitted_died'] >= 0) & (data_lrti['delta_admitted_died'] <= 30)), 'event_lrti_admitted'] = 0\n",
    "\n",
    "#scenario 3\n",
    "#censor deregistered patients\n",
    "data_lrti['delta_admitted_deregistered'] = (data_lrti['deregistered_date'] - data_lrti['admitted_lrti_date']).dt.days\n",
    "data_lrti.loc[data_lrti['delta_admitted_deregistered'] < 0, 'delta_admitted_deregistered'] = np.NaN\n",
    "data_lrti.loc[((data_lrti['delta_admitted_deregistered'] > 0) & (data_lrti['delta_admitted_deregistered'] <= 30)), 'event_lrti_admitted'] = 0\n",
    "\n",
    "#agg scenario 3s\n",
    "data_lrti['delta_admitted_died_deregistered'] = data_lrti['delta_admitted_deregistered'].combine_first(data_lrti['delta_admitted_died'])\n",
    "data_lrti.loc[data_lrti['delta_admitted_died_deregistered'] < 0, 'delta_admitted_died_deregistered'] = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scenario 1\n",
    "#any other patients (nothing happened)\n",
    "data_lrti['event_lrti_admitted'] = data_lrti['event_lrti_admitted'].replace(np.NaN, 0)\n",
    "\n",
    "#assign values for duration column\n",
    "data_lrti['duration_lrti_admitted'] = data_lrti['delta_lrti_admitted'].combine_first(data_lrti['delta_admitted_died_deregistered'])\n",
    "data_lrti['duration_lrti_admitted'] = data_lrti['duration_lrti_admitted'].replace(np.NaN, 30)\n",
    "data_lrti.loc[(data_lrti['duration_lrti_admitted'] > 30), 'duration_lrti_admitted'] = 30\n",
    "\n",
    "#give value 1 to event_lrti_admitted if duration_lrti_admitted is greater or equal to 0 and less than 30\n",
    "data_lrti.loc[((data_lrti['duration_lrti_admitted'] >= 0) & (data_lrti['duration_lrti_admitted'] < 30)), 'event_lrti_admitted'] = 1\n",
    "\n",
    "#drop any rows with value 0 in duration column\n",
    "data_lrti = data_lrti[~(data_lrti['duration_lrti_admitted'] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scenario2 (uncensoring) again to prevent conflict with other scenarios\n",
    "data_lrti.loc[((data_lrti['delta_lrti_admitted'] > 0) & (data_lrti['delta_lrti_admitted'] < 30)), 'event_lrti_admitted'] = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3- dealing with uninteresting and missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop anybody with age less than 18 or 0!\n",
    "data_lrti = data_lrti[~(data_lrti['age'] < 18)] \n",
    "data_lrti['age'][data_lrti['age'] < 0] = np.nan\n",
    "#drop age_cat 0-4 and 5-14\n",
    "data_lrti = data_lrti[data_lrti['age_cat'] != '0-4']\n",
    "data_lrti = data_lrti[data_lrti['age_cat'] != '5-14'] \n",
    "#assign 0 (missingness) to all bmi values less than 10\n",
    "data_lrti['bmi'][data_lrti['bmi'] < 10] = 0\n",
    "#replace 0s in bmi with nans\n",
    "data_lrti['bmi'] = data_lrti['bmi'].replace({0:np.nan})\n",
    "#replace negatives in antibacterial_brit with nans\n",
    "data_lrti['antibacterial_brit'][data_lrti['antibacterial_brit'] < 0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace strings with numerics\n",
    "dict_sex = {'F': 0, 'M': 1}\n",
    "dict_smoking = {'S': 1, 'E': 2, 'N':3, 'M':np.nan}\n",
    "\n",
    "data_lrti = data_lrti.replace({\"sex\": dict_sex})\n",
    "data_lrti = data_lrti.replace({\"smoking\": dict_smoking})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows with missing region\n",
    "if ('region' in data_lrti.columns) and (data_lrti['region'].isnull().sum() > 0):\n",
    "   data_lrti = data_lrti[data_lrti['region'].notna()]\n",
    "\n",
    "#replace rows with missing antibiotics with 0\n",
    "if ('antibacterial_brit' in data_lrti.columns) and (data_lrti['antibacterial_brit'].isnull().sum() > 0):\n",
    "    data_lrti['antibacterial_brit'] = data_lrti['antibacterial_brit'].fillna(0)\n",
    "\n",
    "#replace rows with missing ethnicity with 0\n",
    "if ('ethnicity' in data_lrti.columns) and (data_lrti['ethnicity'].isnull().sum() > 0):\n",
    "    data_lrti['ethnicity'] = data_lrti['ethnicity'].fillna(0)\n",
    "\n",
    "#replace rows with missing smoking with 0\n",
    "if ('smoking' in data_lrti.columns) and (data_lrti['smoking'].isnull().sum() > 0):\n",
    "    data_lrti['smoking'] = data_lrti['smoking'].fillna(0)\n",
    "\n",
    "#replace rows with missing imd with 0\n",
    "if ('imd' in data_lrti.columns) and (data_lrti['imd'].isnull().sum() > 0):\n",
    "    data_lrti['imd'] = data_lrti['imd'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing values of bmi assign 5 and 0; categorise bmi\n",
    "data_lrti['bmi_cat'] = 5\n",
    "\n",
    "for idx, x in enumerate(data_lrti['bmi_cat']):\n",
    "    if data_lrti.iat[idx, data_lrti.columns.get_loc('bmi')] >= 30:\n",
    "        data_lrti.iat[idx, data_lrti.columns.get_loc('bmi_cat')] = 1 #'Obese'\n",
    "    if (data_lrti.iat[idx, data_lrti.columns.get_loc('bmi')] >= 25) and (data_lrti.iat[idx, data_lrti.columns.get_loc('bmi')] < 30):\n",
    "        data_lrti.iat[idx, data_lrti.columns.get_loc('bmi_cat')] = 2 #'Overweight'\n",
    "    if (data_lrti.iat[idx, data_lrti.columns.get_loc('bmi')] >= 18.5) and (data_lrti.iat[idx, data_lrti.columns.get_loc('bmi')] < 25):\n",
    "        data_lrti.iat[idx, data_lrti.columns.get_loc('bmi_cat')] = 3 #'Healthy weight'\n",
    "    if data_lrti.iat[idx, data_lrti.columns.get_loc('bmi')] < 18.5:\n",
    "        data_lrti.iat[idx, data_lrti.columns.get_loc('bmi_cat')] = 4 #'Underweight'\n",
    "\n",
    "if ('bmi_cat' in data_lrti.columns) and (data_lrti['bmi_cat'].isnull().sum() > 0):\n",
    "    data_lrti['bmi_cat'] = data_lrti['bmi_cat'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "##add variable ab_type_cat and assign 0 to its missings\n",
    "#find ab types\n",
    "ab_series = pd.value_counts(data_lrti.ab_type)\n",
    "#take percentages of ab categories\n",
    "ab_category_perc = ab_series/ab_series.sum() * 100\n",
    "#take first 2 categories of abs\n",
    "mask = (ab_category_perc).lt(ab_category_perc[1])\n",
    "#replace category other with categories with 4th percentage or more (less frequent abs)\n",
    "data_lrti['ab_type_cat'] = np.where(data_lrti['ab_type'].isin(ab_series[mask].index),'other',data_lrti['ab_type'])\n",
    "#assign no to non ab users\n",
    "data_lrti.loc[data_lrti['lrti_ab_date'].isnull(), 'ab_type_cat'] = 'no'\n",
    "#fill nas with 0 and then assign other to 0s\n",
    "data_lrti['ab_type_cat'] = data_lrti['ab_type_cat'].fillna(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-4- translate values of categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define dictionaries\n",
    "dict_sex = {0: 'female', 1: 'male'}\n",
    "dict_ethnicity = {1.0: 'white', 2.0: 'mixed', 3.0: 'asian', 4.0: 'black', 5.0: 'other', 0: 'unknown'}\n",
    "dict_smoking = {1.0:'smoker', 2.0:'ex_smoker', 3.0:'never_smoked', 0: 'unknown'}\n",
    "dict_imd = {1:'very_affluent', 2:'affluent', 3:'medium', 4:'unaffluent', 5:'very_unaffluent', 0:'unknown'}\n",
    "dict_bmi = {1:'obese', 2:'overweight', 3:'healthy_weight', 4:'underweight', 5:'unknown', 0:'unknown'}\n",
    "dict_flu = {0: 'no', 1: 'yes'}\n",
    "dict_region = {'London': 'london', 'North East': 'north_east', 'North West': 'north_west', 'East': 'east', 'West Midlands': 'west_midlands', 'Yorkshire and The Humber': 'yorkshire', 'South East': 'south_east', 'East Midlands': 'east_midlands', 'South West': 'south_west'}\n",
    "dict_age = {'0-4':'0_4', '5-14':'5_14', '15-24':'15_24', '25-34':'25_34', '35-44':'35_44', '45-54':'45_54', '55-64':'55_64', '65-74':'65_74', '75+':'75_more'}\n",
    "dict_ab_type = {0:'other'}\n",
    "\n",
    "#reoplace values of dictionaries with existing ones\n",
    "data_lrti = data_lrti.replace({\"sex\": dict_sex})\n",
    "data_lrti = data_lrti.replace({\"ethnicity\": dict_ethnicity})\n",
    "data_lrti = data_lrti.replace({\"smoking\": dict_smoking})\n",
    "data_lrti = data_lrti.replace({\"imd\": dict_imd})\n",
    "data_lrti = data_lrti.replace({\"bmi_cat\": dict_bmi})\n",
    "data_lrti = data_lrti.replace({\"flu_vaccine\": dict_flu})\n",
    "data_lrti = data_lrti.replace({\"region\": dict_region})\n",
    "data_lrti = data_lrti.replace({\"age_cat\": dict_age})\n",
    "data_lrti = data_lrti.replace({\"ab_type_cat\": dict_ab_type})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "white      0.599405\n",
       "unknown    0.250143\n",
       "other      0.076308\n",
       "asian      0.074145\n",
       "Name: ethnicity, dtype: float64"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lrti.ethnicity.value_counts('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unknown         0.415125\n",
       "smoker          0.390975\n",
       "never_smoked    0.129574\n",
       "ex_smoker       0.064326\n",
       "Name: smoking, dtype: float64"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lrti.smoking.value_counts('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "obese             0.520006\n",
       "unknown           0.257131\n",
       "overweight        0.110310\n",
       "healthy_weight    0.081093\n",
       "underweight       0.031460\n",
       "Name: bmi_cat, dtype: float64"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lrti.bmi_cat.value_counts('NA')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-5- categorising and assigning max value for continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorise CCI\n",
    "data_lrti['CCI_cat'] = pd.cut(data_lrti['CCI'], right=False, bins=[0,1,3,5,7,35], labels=['very_low', 'low', 'medium', 'high', 'very_high'])\n",
    "\n",
    "#assign max value to outliers (beyond 95 percentile)\n",
    "data_lrti['antibacterial_brit'].clip(0, data_lrti['antibacterial_brit'].quantile(0.95), inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "sns.heatmap(data_lrti[['age', 'bmi', 'CCI', 'antibacterial_brit']].corr(), \n",
    "            annot=True, fmt=\".2f\", annot_kws={\"size\":12},\n",
    "            vmin=-1.0, vmax=1.0)\n",
    "\n",
    "ax.set_xticklabels(['Age', 'BMI', 'CCI', 'Antibacterial prescriptions'], rotation=90, fontsize=9)\n",
    "ax.set_yticklabels(['Age', 'BMI', 'CCI', 'Antibacterial prescriptions',], rotation=0, fontsize=9)                        \n",
    "plt.tight_layout()\n",
    "plt.savefig('../output/hospitalisation_prediction_lrti/corr_lrti.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- separate data of incident/prevalent and with/without antibiotics and stratified sub-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lrti = data_lrti[['age_cat', 'sex', 'CCI_cat', 'flu_vaccine', 'bmi_cat', 'region', 'imd', 'ethnicity', 'smoking', 'season', \n",
    "                                   'antibacterial_brit', 'lrti_ab_date', 'ab_type_cat', 'incdt_lrti_date', 'date', 'period',\n",
    "                                   'event_lrti_admitted', 'duration_lrti_admitted'\n",
    "                                    ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#incident and prevalent infection \n",
    "data_lrti_incdt = data_lrti[data_lrti['incdt_lrti_date'] == 0]\n",
    "data_lrti_prevl = data_lrti[data_lrti['incdt_lrti_date'] == 1]\n",
    "#switch variable lrti_ab_date into a binary variable\n",
    "data_lrti_incdt[['lrti_ab_date']] = data_lrti_incdt[['lrti_ab_date']].where(data_lrti_incdt[['lrti_ab_date']].isnull(), 1).fillna(0).astype(int)\n",
    "data_lrti_prevl[['lrti_ab_date']] = data_lrti_prevl[['lrti_ab_date']].where(data_lrti_prevl[['lrti_ab_date']].isnull(), 1).fillna(0).astype(int)\n",
    "\n",
    "#incident and prevalent infection stratified by sex categories\n",
    "data_lrti_incdt_males = data_lrti_incdt[data_lrti_incdt['sex'] == 'male']\n",
    "data_lrti_incdt_females = data_lrti_incdt[data_lrti_incdt['sex'] == 'female']\n",
    "data_lrti_prevl_males = data_lrti_prevl[data_lrti_prevl['sex'] == 'male']\n",
    "data_lrti_prevl_females = data_lrti_prevl[data_lrti_prevl['sex'] == 'female']\n",
    "\n",
    "#incident and prevalent infection stratified by age categories\n",
    "data_lrti_incdt_15_24 = data_lrti_incdt[data_lrti_incdt['age_cat'] == '15_24']\n",
    "data_lrti_incdt_25_34 = data_lrti_incdt[data_lrti_incdt['age_cat'] == '25_34']\n",
    "data_lrti_incdt_35_44 = data_lrti_incdt[data_lrti_incdt['age_cat'] == '35_44']\n",
    "data_lrti_incdt_45_54 = data_lrti_incdt[data_lrti_incdt['age_cat'] == '45_54']\n",
    "data_lrti_incdt_55_64 = data_lrti_incdt[data_lrti_incdt['age_cat'] == '55_64']\n",
    "data_lrti_incdt_65_74 = data_lrti_incdt[data_lrti_incdt['age_cat'] == '65_74']\n",
    "data_lrti_incdt_75_more = data_lrti_incdt[data_lrti_incdt['age_cat'] == '75_more']\n",
    "data_lrti_prevl_15_24 = data_lrti_prevl[data_lrti_prevl['age_cat'] == '15_24']\n",
    "data_lrti_prevl_25_34 = data_lrti_prevl[data_lrti_prevl['age_cat'] == '25_34']\n",
    "data_lrti_prevl_35_44 = data_lrti_prevl[data_lrti_prevl['age_cat'] == '35_44']\n",
    "data_lrti_prevl_45_54 = data_lrti_prevl[data_lrti_prevl['age_cat'] == '45_54']\n",
    "data_lrti_prevl_55_64 = data_lrti_prevl[data_lrti_prevl['age_cat'] == '55_64']\n",
    "data_lrti_prevl_65_74 = data_lrti_prevl[data_lrti_prevl['age_cat'] == '65_74']\n",
    "data_lrti_prevl_75_more = data_lrti_prevl[data_lrti_prevl['age_cat'] == '75_more']\n",
    "\n",
    "#incident and prevalent infection stratified by time period categories\n",
    "data_lrti_incdt_prepandemic = data_lrti_incdt[data_lrti_incdt['date'] <= '2019-12']\n",
    "data_lrti_incdt_during_pandemic = data_lrti_incdt[(data_lrti_incdt['date'] >= '2020-03') & (data_lrti_incdt['date']<= '2021-03')]\n",
    "data_lrti_incdt_post_2nd_lockdown = data_lrti_incdt[data_lrti_incdt['date'] <= '2021-04']\n",
    "data_lrti_prevl_prepandemic = data_lrti_prevl[data_lrti_prevl['date'] <= '2019-12']\n",
    "data_lrti_prevl_during_pandemic = data_lrti_prevl[(data_lrti_prevl['date'] >= '2020-03') & (data_lrti_prevl['date']<= '2021-03')]\n",
    "data_lrti_prevl_post_2nd_lockdown = data_lrti_prevl[data_lrti_prevl['date'] <= '2021-04']\n",
    "\n",
    "# no antibiotics and incident hospital admission\n",
    "data_lrti_no_abs_incdt = data_lrti[data_lrti['lrti_ab_date'].isnull()]\n",
    "data_lrti_no_abs_incdt = data_lrti_no_abs_incdt[data_lrti_no_abs_incdt['incdt_lrti_date'] == 0]\n",
    "\n",
    "# with antibiotics and incident hospital admission\n",
    "data_lrti_abs_incdt = data_lrti[data_lrti['lrti_ab_date'].notnull()]\n",
    "data_lrti_abs_incdt = data_lrti_abs_incdt[data_lrti_abs_incdt['incdt_lrti_date'] == 0]\n",
    "\n",
    "# no antibiotics and prevalent hospital admission\n",
    "data_lrti_no_abs_prevl = data_lrti[data_lrti['lrti_ab_date'].isnull()]\n",
    "data_lrti_no_abs_prevl = data_lrti_no_abs_prevl[data_lrti_no_abs_prevl['incdt_lrti_date'] == 1]\n",
    "\n",
    "# with antibiotics and prevalent hospital admission\n",
    "data_lrti_abs_prevl = data_lrti[data_lrti['lrti_ab_date'].notnull()]\n",
    "data_lrti_abs_prevl = data_lrti_abs_prevl[data_lrti_abs_prevl['incdt_lrti_date'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'data_lrti' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "#store data for plotting hosp admission counts and percentages\n",
    "%store data_lrti\n",
    "# save data\n",
    "data_lrti.to_csv('../output/hospitalisation_prediction_lrti/data_lrti.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5- cox modelling for hospital admissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build cox models\n",
    "def rf_lr_model_build(infection, infection_type, data):\n",
    "\n",
    "    ##data prep\n",
    "    #keep relevant columns for encoder\n",
    "    # data = data.head(100000)\n",
    "    data_req = data[['age_cat', 'sex', 'CCI_cat', 'flu_vaccine', 'region', 'imd', 'ethnicity', 'smoking', 'season', 'bmi_cat', 'antibacterial_brit', infection+'_ab_date']]\n",
    "    data_req_df = pd.DataFrame(data_req) \n",
    "    #creating instance of one-hot-encoder\n",
    "    enc = OneHotEncoder()\n",
    "    data_req_enc = pd.DataFrame(enc.fit_transform(data_req_df[['age_cat', 'sex', 'CCI_cat', 'flu_vaccine', 'region', 'imd', 'ethnicity', 'smoking', 'season', 'bmi_cat']]).toarray())\n",
    "    data_req_enc.columns = enc.get_feature_names(data_req_df[['age_cat', 'sex', 'CCI_cat', 'flu_vaccine', 'region', 'imd', 'ethnicity', 'smoking', 'season', 'bmi_cat']].columns)\n",
    "    #concat binarised and already binary columns\n",
    "    data_req_df_cont = data_req_df[['antibacterial_brit', infection+'_ab_date']].reset_index(drop=True)\n",
    "    data_final = pd.concat([data_req_enc, data_req_df_cont], axis=1)\n",
    "                        \n",
    "    #keep needed columns, drop ref categories\n",
    "    data_col =['age_cat_25_34', 'age_cat_35_44', 'age_cat_45_54', 'age_cat_55_64', 'age_cat_65_74', 'age_cat_75_more', #'age_cat_15_24', \n",
    "               'sex_female', #'sex_male', \n",
    "               'CCI_cat_high', 'CCI_cat_low', 'CCI_cat_medium', 'CCI_cat_very_high', #'CCI_cat_very_low', \n",
    "               'flu_vaccine_yes', #'flu_vaccine_no', \n",
    "               'region_east', 'region_east_midlands', 'region_london', 'region_north_east', 'region_north_west', 'region_south_east', 'region_south_west', 'region_west_midlands', #'region_yorkshire', \n",
    "               'imd_affluent', 'imd_unaffluent', 'imd_unknown', 'imd_very_affluent', 'imd_very_unaffluent', #'imd_medium',\n",
    "               'ethnicity_mixed', 'ethnicity_asian', 'ethnicity_black', 'ethnicity_other', 'ethnicity_unknown', #'ethnicity_white', \n",
    "               'smoking_ex_smoker', 'smoking_smoker', 'smoking_unknown', #'smoking_never_smoked',\n",
    "               'season_autumn', 'season_summer', 'season_winter', #'season_spring',\n",
    "               'bmi_cat_obese', 'bmi_cat_overweight', 'bmi_cat_underweight', 'bmi_cat_unknown', #'bmi_cat_healthy_weight',\n",
    "               'antibacterial_brit'\n",
    "               ]\n",
    "    \n",
    "    #randomly splitting data into training (%75) and testing (%25)\n",
    "    data_dev, data_val = train_test_split(data_final, test_size=0.25, random_state=42)\n",
    "    data_dev_y = data_dev[infection+'_ab_date']\n",
    "    data_dev_x = data_dev[data_col]\n",
    "    data_val_y = data_val[infection+'_ab_date']\n",
    "    data_val_x = data_val[data_col]\n",
    "\n",
    "    #grid search for rf modelling\n",
    "    parameters = {'n_estimators': (150,200,250,300,350),\n",
    "                  'criterion': ('gini', 'entropy'),\n",
    "                  'max_depth': (10,15,20,25,30),\n",
    "                  'max_features': ('auto', 'sqrt'),\n",
    "                  'max_leaf_nodes': (25,30,35,40,45),\n",
    "                #   'min_samples_split': (2,4,6)\n",
    "                  }\n",
    "    rf_grid = GridSearchCV(RandomForestClassifier(n_jobs = -1, oob_score=False), param_grid=parameters, cv=3, verbose=True)\n",
    "    rf_grid_fit = rf_grid.fit(data_dev_x, data_dev_y)\n",
    "    #build rf model with best parameters found by grid search\n",
    "    rf = rf_grid_fit.best_estimator_\n",
    "    rf\n",
    "\n",
    "    ##building rf model\n",
    "    # rf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=50, max_depth=25)\n",
    "    rf.fit(data_dev_x, data_dev_y)\n",
    "    #predict probabilities to rf model\n",
    "    rf_pred_dev = rf.predict_proba(data_dev_x)[:,1]\n",
    "    rf_pred_val = rf.predict_proba(data_val_x)[:,1]\n",
    "\n",
    "    ##building lr model\n",
    "    lr_uncon = sm.Logit(endog=data_dev_y, exog=data_dev_x)\n",
    "    lr_uncon = lr_uncon.fit(disp=0)\n",
    "    #predict probabilities of lr model\n",
    "    lr_uncon_pred_dev = lr_uncon.predict(data_dev_x)\n",
    "    lr_uncon_pred_val = lr_uncon.predict(data_val_x)\n",
    "\n",
    "    #print aurocs\n",
    "    auroc_rf_dev = roc_auc_score(data_dev_y, rf_pred_dev)\n",
    "    auroc_rf_val = roc_auc_score(data_val_y, rf_pred_val)\n",
    "    auroc_lr_uncon_dev = sklearn.metrics.roc_auc_score(data_dev_y, lr_uncon_pred_dev)\n",
    "    auroc_lr_uncon_val = sklearn.metrics.roc_auc_score(data_val_y, lr_uncon_pred_val)\n",
    "    print('RF AUROC with development and validation data: %.2f'%(auroc_rf_dev), 'and %.2f' % (auroc_rf_val))\n",
    "    print('LR AUROC with development and validation data: %.2f'%(auroc_lr_uncon_dev), 'and %.2f' % (auroc_lr_uncon_val))\n",
    "    \n",
    "    #plot roc curves\n",
    "    r_fpr_rf_dev, r_tpr_rf_dev, _ = roc_curve(data_dev_y, rf_pred_dev)\n",
    "    r_fpr_rf_val, r_tpr_rf_val, _ = roc_curve(data_val_y, rf_pred_val)\n",
    "    r_fpr_lr_uncon_dev, r_tpr_lr_uncon_dev, _ = roc_curve(data_dev_y, lr_uncon_pred_dev)\n",
    "    r_fpr_lr_uncon_val, r_tpr_lr_uncon_val, _ = roc_curve(data_val_y, lr_uncon_pred_val)\n",
    "\n",
    "    fig, ax1 = pyplot.subplots(figsize=(7, 7))\n",
    "    line_rf_dev, = plt.plot(r_fpr_rf_dev, r_tpr_rf_dev, linestyle='-', marker='o', markersize=6, markevery=0.1, color='#8D576D', label='RF with development data')\n",
    "    line_rf_val, = plt.plot(r_fpr_rf_val, r_tpr_rf_val, linestyle='--', marker='v', markersize=6, markevery=0.1, color='#eb91b7', label='RF with validation data')\n",
    "    line_lr_dev, = plt.plot(r_fpr_lr_uncon_dev, r_tpr_lr_uncon_dev, linestyle='-', marker='o', markersize=6, markevery=0.1, color='#408678', label='LR with development data')\n",
    "    line_lr_val, = plt.plot(r_fpr_lr_uncon_val, r_tpr_lr_uncon_val, linestyle='--', marker='v', markersize=6, markevery=0.1, color='#6ce0c9', label='LR with validation data')\n",
    "    \n",
    "    squares = [0,1.01]\n",
    "    plt.plot(squares,linewidth=1, color='grey')\n",
    "    plt.ylim(0,1.01)\n",
    "    plt.xlim(0,1)\n",
    "    plt.xlabel('Specificity', fontsize=14)\n",
    "    plt.ylabel('Sensitivity', fontsize=14)\n",
    "\n",
    "    #reversing xticks\n",
    "    xticks = [1.0, 0.8, 0.6, 0.4, 0.2, 0.0]\n",
    "    x = np.arange(len(xticks))\n",
    "    ax1.set(xticklabels=xticks)\n",
    "    ax1.legend(fontsize=12)\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10, rotation=0)\n",
    "    plt.title(\"ROC curve of RF and LR models\", fontsize=14)\n",
    "    plt.savefig('../output/hospitalisation_prediction_'+infection+'/roc_'+infection+'_'+infection_type+'.jpg', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    ##calibration\n",
    "    rf_prob_pred_dev = pd.DataFrame(data=rf_pred_dev, columns=['pred'])\n",
    "    #group cumulative hazard into 10 bins with equal frequency of observations in each bin\n",
    "    rf_prob_pred_dev['bins'] = pd.qcut(rf_prob_pred_dev['pred'], 10, duplicates=\"drop\")\n",
    "    #merge cumulative hazards with their actual events (0, 1)\n",
    "    rf_prob_pred_dev = pd.merge(rf_prob_pred_dev, data_dev[infection+'_ab_date'], left_index=True, right_index=True)\n",
    "    #groupby bins to find mean predicted probability for each bin (pred_mean), count of events in each bin (event_sum) and count of samples in each bin (event_count)\n",
    "    rf_prob_pred_dev_groupby_bin = rf_prob_pred_dev.groupby('bins')[['pred', infection+'_ab_date']].agg(['mean', 'sum', 'count']).reset_index()\n",
    "    rf_prob_pred_dev_groupby_bin.columns = ['bins', 'pred_mean', 'pred_sum', 'pred_count', infection+'_ab_date_mean', infection+'_ab_date_sum', infection+'_ab_date_count']\n",
    "    #calculate proportion of events in each bin\n",
    "    rf_prob_pred_dev_groupby_bin[infection+'_ab_date_proportion'] = rf_prob_pred_dev_groupby_bin[infection+'_ab_date_sum']/rf_prob_pred_dev_groupby_bin[infection+'_ab_date_count']\n",
    "    \n",
    "    ##calibration for RF\n",
    "    rf_prob_pred_val = pd.DataFrame(data=rf_pred_val, columns=['pred'])\n",
    "    #group cumulative hazard into 10 bins with equal frequency of observations in each bin\n",
    "    rf_prob_pred_val['bins'] = pd.qcut(rf_prob_pred_val['pred'], 10, duplicates=\"drop\")\n",
    "    #merge cumulative hazards with their actual events (0, 1)\n",
    "    rf_prob_pred_val = pd.merge(rf_prob_pred_val, data_val[infection+'_ab_date'], left_index=True, right_index=True)\n",
    "    #groupby bins to find mean predicted probability for each bin (pred_mean), count of events in each bin (event_sum) and count of samples in each bin (event_count)\n",
    "    rf_prob_pred_val_groupby_bin = rf_prob_pred_val.groupby('bins')[['pred', infection+'_ab_date']].agg(['mean', 'sum', 'count']).reset_index()\n",
    "    rf_prob_pred_val_groupby_bin.columns = ['bins', 'pred_mean', 'pred_sum', 'pred_count', infection+'_ab_date_mean', infection+'_ab_date_sum', infection+'_ab_date_count']\n",
    "    #calculate proportion of events in each bin\n",
    "    rf_prob_pred_val_groupby_bin[infection+'_ab_date_proportion'] = rf_prob_pred_val_groupby_bin[infection+'_ab_date_sum']/rf_prob_pred_val_groupby_bin[infection+'_ab_date_count']\n",
    "    #plot calibration plot for RF model with development and validation data\n",
    "    fig, ax1 = plt.subplots(figsize=(7, 7))\n",
    "    plt.plot(rf_prob_pred_dev_groupby_bin.pred_mean, rf_prob_pred_dev_groupby_bin[infection+'_ab_date_proportion'], color='#8D576D', linestyle='solid', marker='o', alpha=0.9)\n",
    "    plt.plot(rf_prob_pred_val_groupby_bin.pred_mean, rf_prob_pred_val_groupby_bin[infection+'_ab_date_proportion'], color='#eb91b7', linestyle='dashed', marker='v', alpha=0.9)\n",
    "    plt.xlabel('Mean predicted probabilities', fontsize=14)\n",
    "    plt.ylabel('Proportion of observed values', fontsize=14)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12, rotation=0)\n",
    "    plt.xlim(0, max(rf_prob_pred_dev_groupby_bin.pred_mean.max(), rf_prob_pred_dev_groupby_bin[infection+'_ab_date_proportion'].max()).round(decimals = 2) + (max(rf_prob_pred_dev_groupby_bin.pred_mean.max(), rf_prob_pred_dev_groupby_bin[infection+'_ab_date_proportion'].max()).round(decimals = 2)/3))\n",
    "    plt.ylim(0, max(rf_prob_pred_dev_groupby_bin.pred_mean.max(), rf_prob_pred_dev_groupby_bin[infection+'_ab_date_proportion'].max()).round(decimals = 2) + (max(rf_prob_pred_dev_groupby_bin.pred_mean.max(), rf_prob_pred_dev_groupby_bin[infection+'_ab_date_proportion'].max()).round(decimals = 2)/3))\n",
    "    plt.plot([0, 1], [0, 1], linewidth=1, linestyle='-', color='grey')\n",
    "    plt.title(\"Calibration plot of RF model\", fontsize=14)\n",
    "    legend_dev = mlines.Line2D([], [], color='#8D576D', linestyle='-', marker='o', markersize=10, label='Development data', alpha=.9)\n",
    "    legend_val = mlines.Line2D([], [], color='#eb91b7', linestyle='--', marker='v', markersize=10, label='Validation data', alpha=.9)\n",
    "    plt.legend(handles=[legend_dev, legend_val])\n",
    "    plt.savefig('../output/hospitalisation_prediction_'+infection+'/calib_rf_'+infection+'_'+infection_type+'.jpg', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    ##calibration for LR\n",
    "    lr_uncon_prob_pred_dev = pd.DataFrame(data=lr_uncon_pred_dev, columns=['pred'])\n",
    "    #group cumulative hazard into 10 bins with equal frequency of observations in each bin\n",
    "    lr_uncon_prob_pred_dev['bins'] = pd.qcut(lr_uncon_prob_pred_dev['pred'], 10, duplicates=\"drop\")\n",
    "    #merge cumulative hazards with their actual events (0, 1)\n",
    "    lr_uncon_prob_pred_dev = pd.merge(lr_uncon_prob_pred_dev, data_dev[infection+'_ab_date'], left_index=True, right_index=True)\n",
    "    #groupby bins to find mean predicted probability for each bin (pred_mean), count of events in each bin (event_sum) and count of samples in each bin (event_count)\n",
    "    lr_uncon_prob_pred_dev_groupby_bin = lr_uncon_prob_pred_dev.groupby('bins')[['pred', infection+'_ab_date']].agg(['mean', 'sum', 'count']).reset_index()\n",
    "    lr_uncon_prob_pred_dev_groupby_bin.columns = ['bins', 'pred_mean', 'pred_sum', 'pred_count', infection+'_ab_date_mean', infection+'_ab_date_sum', infection+'_ab_date_count']\n",
    "    #calculate proportion of events in each bin\n",
    "    lr_uncon_prob_pred_dev_groupby_bin[infection+'_ab_date_proportion'] = lr_uncon_prob_pred_dev_groupby_bin[infection+'_ab_date_sum']/lr_uncon_prob_pred_dev_groupby_bin[infection+'_ab_date_count']\n",
    "    #\n",
    "    lr_uncon_prob_pred_val = pd.DataFrame(data=lr_uncon_pred_val, columns=['pred'])\n",
    "    #group cumulative hazard into 10 bins with equal frequency of observations in each bin\n",
    "    lr_uncon_prob_pred_val['bins'] = pd.qcut(lr_uncon_prob_pred_val['pred'], 10, duplicates=\"drop\")\n",
    "    #merge cumulative hazards with their actual events (0, 1)\n",
    "    lr_uncon_prob_pred_val = pd.merge(lr_uncon_prob_pred_val, data_val[infection+'_ab_date'], left_index=True, right_index=True)\n",
    "    #groupby bins to find mean predicted probability for each bin (pred_mean), count of events in each bin (event_sum) and count of samples in each bin (event_count)\n",
    "    lr_uncon_prob_pred_val_groupby_bin = lr_uncon_prob_pred_val.groupby('bins')[['pred', infection+'_ab_date']].agg(['mean', 'sum', 'count']).reset_index()\n",
    "    lr_uncon_prob_pred_val_groupby_bin.columns = ['bins', 'pred_mean', 'pred_sum', 'pred_count', infection+'_ab_date_mean', infection+'_ab_date_sum', infection+'_ab_date_count']\n",
    "    #calculate proportion of events in each bin\n",
    "    lr_uncon_prob_pred_val_groupby_bin[infection+'_ab_date_proportion'] = lr_uncon_prob_pred_val_groupby_bin[infection+'_ab_date_sum']/lr_uncon_prob_pred_val_groupby_bin[infection+'_ab_date_count']\n",
    "    #plot calibration plot for RF model with development and validation data\n",
    "    fig, ax1 = plt.subplots(figsize=(7, 7))\n",
    "    plt.plot(lr_uncon_prob_pred_dev_groupby_bin.pred_mean, lr_uncon_prob_pred_dev_groupby_bin[infection+'_ab_date_proportion'], color='#408678', linestyle='solid', marker='o', alpha=0.9)\n",
    "    plt.plot(lr_uncon_prob_pred_val_groupby_bin.pred_mean, lr_uncon_prob_pred_val_groupby_bin[infection+'_ab_date_proportion'], color='#6ce0c9', linestyle='dashed', marker='v', alpha=0.6)\n",
    "    plt.xlabel('Mean predicted probabilities', fontsize=14)\n",
    "    plt.ylabel('Proportion of observed values', fontsize=14)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12, rotation=0)\n",
    "    plt.xlim(0, max(lr_uncon_prob_pred_dev_groupby_bin.pred_mean.max(), lr_uncon_prob_pred_dev_groupby_bin[infection+'_ab_date_proportion'].max()).round(decimals = 2) + (max(lr_uncon_prob_pred_dev_groupby_bin.pred_mean.max(), lr_uncon_prob_pred_dev_groupby_bin[infection+'_ab_date_proportion'].max()).round(decimals = 2)/3))\n",
    "    plt.ylim(0, max(lr_uncon_prob_pred_dev_groupby_bin.pred_mean.max(), lr_uncon_prob_pred_dev_groupby_bin[infection+'_ab_date_proportion'].max()).round(decimals = 2) + (max(lr_uncon_prob_pred_dev_groupby_bin.pred_mean.max(), lr_uncon_prob_pred_dev_groupby_bin[infection+'_ab_date_proportion'].max()).round(decimals = 2)/3))\n",
    "    plt.plot([0, 1], [0, 1], linewidth=1, linestyle='-', color='grey')\n",
    "    plt.title(\"Calibration plot of LR model\", fontsize=14)\n",
    "    legend_dev = mlines.Line2D([], [], color='#408678', linestyle='-', marker='o', markersize=10, label='Development data', alpha=.9)\n",
    "    legend_val = mlines.Line2D([], [], color='#6ce0c9', linestyle='--', marker='v', markersize=10, label='Validation data', alpha=.6)\n",
    "    plt.legend(handles=[legend_dev, legend_val])\n",
    "    plt.savefig('../output/hospitalisation_prediction_'+infection+'/calib_lr_'+infection+'_'+infection_type+'.jpg', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    #print feature importance of RF model\n",
    "    rf_coef = pd.DataFrame(data=rf.feature_importances_, index=data_dev_x.columns, columns=['rf_feature_importance'])#.sort_values(by=['feature_importance'], ascending=False)\n",
    "    rf_coef = rf_coef.sort_values('rf_feature_importance', ascending=False)\n",
    "    # print(\"\\nRF feature importance:\\n\", rf_coef)\n",
    "\n",
    "    #print summary of LR model\n",
    "    lr_uncon_coef = pd.DataFrame(data=lr_uncon.params, index=data_dev_x.columns, columns=['lr_coef'])\n",
    "#     lr_uncon_coef = lr_uncon_coef.abs().sort_values('lr_feature_importance', ascending=False)\n",
    "    lr_uncon_coef = lr_uncon_coef.sort_values('lr_coef', ascending=False)\n",
    "    # print(\"\\nLR fcoefficients:\\n\", lr_uncon_coef)\n",
    "\n",
    "    #concat coefs and save \n",
    "    coefs = pd.concat([rf_coef, lr_uncon_coef], axis=1)\n",
    "    coefs.to_csv('../output/hospitalisation_prediction_'+infection+'/coefs_'+infection+'_'+infection_type+'.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-1- incident lrti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build rf and lr models\n",
    "rf_lr_model_build('lrti', 'incdt', data_lrti_incdt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-2- prevalent lrti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build rf and lr models\n",
    "rf_lr_model_build('lrti', 'prevl', data_lrti_incdt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aab3e9328fa5f6e836343e29403fbe12ff1c7623340021c445797f56a1eab521"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
