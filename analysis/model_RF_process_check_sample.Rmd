---
title: "random sampling by subclass"
---

```{r setup, include=FALSE}
library(knitr)
library(dplyr)
library(skimr)
library('tidyverse')

library("ggplot2")

library(caret)
knitr::opts_chunk$set(echo = TRUE)
options(knitr.table.format="html")
```



# import dataset
```{r}
df <- read_rds("/Users/yayang/Documents/GitHub/amr-uom-brit/output/matched_ab.rds")
#df <- read_rds(here::here("output","matched_ab.rds"))
count(df)

df=df[!is.na(df$case),]
count(df)
table(df$case)

# sample by subgroup

df =df %>% group_by(subclass,case) %>% sample_n(1)
df=df%>%ungroup(subclass,case)
count(df)
table(df$case)


#df=df%>% filter(total_ab>0)

```

check variables
```{r}
str(df)
df$exposure_period=-df$exposure_period
df$case= ifelse(df$case=="1","case","control")# for model training

```

handle missing value (replace with 0 = no ab use)
```{r}
df[is.na(df)] <- "0"

#add indicator for missing value
df$prescribe_time_0=ifelse(is.na(df$prescribe_times),0,1) #1= ab user ,0=non 
df$prescribe_time_1=ifelse(df$prescribe_times==1,1,0) 
df$prescribe_time_2=ifelse(df$prescribe_times==2,1,0) 
df$prescribe_time_3=ifelse(df$prescribe_times>=3,1,0) # missing SD, CV
names(df)
```


training vs. validation
```{r }
# develop model

set.seed(123)

df=sample_n(df,1000)

y=df$case

table(y)
```


```{r }
set.seed(0)
all_idx <- 1:nrow(df)

train_idx <- sample(all_idx, nrow(df) * 0.8)
#valid_idx <- sample(all_idx[!all_idx %in% train_idx], nrow(df) * 0.2)
test_idx <- all_idx[!all_idx %in% train_idx]

pred_col=c( "case",
                 "AB_1_type","AB_6wk_type" , "ab_6w_binary" ,
             "AB_6wk" ,"total_ab", "ab_prescriptions","ab_types",
              "prescribe_times","exposure_period"  ,
               "recent_ab_days",  
              "broad_prop" ,"broad_ab_prescriptions",
             "interval_med" ,  "interval_mean" , "interval_sd" , "interval_CV",
              "length_med" , "length_mean", "length_sd" ,  "length_CV","prescribe_time_0","prescribe_time_1","prescribe_time_2","prescribe_time_3")
  
train_X <- df[train_idx,pred_col]
#valid_X <- df[valid_idx,]
test_X <- df[test_idx,pred_col]

train_Y <- df[train_idx,"case"]
#valid_Y <- subdat[valid_idx,"LVD"]
test_Y <- df[test_idx,"case"]

```


# train model
https://stackoverflow.com/questions/57939453/building-a-randomforest-with-caret 

# tune parameter
```{r}
set.seed(1234)
#cv_folds <- createFolds(df$case, k = 5, returnTrain = TRUE)


# create tune control
#mtry <- sqrt(ncol(x))
tuneGrid <- expand.grid(.mtry = c(1 : 10))

ctrl <- trainControl(
                    # method = "cv",
                     #number = 5,
                     search = 'grid',
                     classProbs = TRUE,
                     savePredictions = TRUE,
                    # index = cv_folds,
                     summaryFunction = twoClassSummary) #in most cases a better summary for two class problem


# other tuning parameters
ntrees <- c(500, 1000)    
nodesize <- c(1, 5)

params <- expand.grid(ntrees = ntrees,
                      nodesize = nodesize)
```

```{r}
store_maxnode <- vector("list", nrow(params))

for(i in 1:nrow(params)){
  
  nodesize <- params[i,2]
  ntree <- params[i,1]
  
  set.seed(65)
  
  rf_model <- train(case~.,
                       data = train_X,
                       method = "rf",
                       importance=TRUE,
                       metric = "ROC",
                       tuneGrid = tuneGrid,
                       trControl = ctrl,
                       ntree = ntree,
                       nodesize = nodesize)
  
  store_maxnode[[i]] <- rf_model
}


names(store_maxnode) <- paste("ntrees:", params$ntrees,
                              "nodesize:", params$nodesize)

results_mtry <- resamples(store_maxnode)

summary(results_mtry)
lapply(store_maxnode, function(x) x$best) #get the best mtry for each model
lapply(store_maxnode, function(x) x$results[x$results$ROC == max(x$results$ROC),])#get the best average performance for each model
```












```{r eval=FALSE, include=FALSE}
#df <- read_rds("/Users/yayang/Documents/GitHub/amr-uom-brit/output/matched_ab.rds")
df <- read_rds(here::here("output","matched_ab.rds"))
count(df)

df=df[!is.na(df$case),]
count(df)
table(df$case)

df_uniq= distinct(df)
count(df_uniq)
table(df_uniq$case)


```


```{r eval=FALSE, include=FALSE}
#DF <- read_rds("/Users/yayang/Documents/GitHub/amr-uom-brit/output/matched_outcome.rds")
DF<- read_rds(here::here("output","matched_outcome.rds"))
DF=DF%>% select("patient_id","patient_index_date","case","age","wave","sex","age_cat","region","subclass")
count(DF)
table(DF$case)


DF=DF[!is.na(DF$case),]
count(DF)
table(DF$case)


DF_uniq=DF%>% distinct(patient_id,patient_index_date, .keep_all=T)
count(DF_uniq)
table(DF_uniq$case)
# filter only 1 ID
#df=df %>% distinct(patient_id, .keep_all=T)
#data=merge(df,DF,by=c("patient_id","patient_index_date"), all.x = TRUE)

#df=data
#rm(DF,data)

#table(df$case)



DF_sub = DF%>% group_by(subclass,case) %>% sample_n(1)
DF_sub=DF_sub%>%ungroup(subclass,case)
count(DF_sub)
table(DF_sub$case)
```


```{r eval=FALSE, include=FALSE}
DF$case=as.factor(DF$case)
DF$sex=as.factor(DF$sex)

#boxplot(age ~ case, data = DF)
densityplot(~ wave, data = DF, groups = case)
densityplot(~ age, data = DF, groups = case)

tbl1=table(DF$sex,DF$case)
tbl2=table(DF$region,DF$case)

prop.table(tbl1, 2)
prop.table(tbl2, 2)

```


# distinct
```{r eval=FALSE, include=FALSE}
DF_uniq$case=as.factor(DF_uniq$case)
DF_uniq$sex=as.factor(DF_uniq$sex)

densityplot(~ wave, data = DF_uniq, groups = case)
densityplot(~ age, data = DF_uniq, groups = case)

tbl1=table(DF_uniq$sex,DF_uniq$case) 
tbl2=table(DF_uniq$region,DF_uniq$case)


prop.table(tbl1, 2)
prop.table(tbl2, 2)

```


# random selection
```{r eval=FALSE, include=FALSE}
DF_sub$case=as.factor(DF_sub$case)
DF_sub$sex=as.factor(DF_sub$sex)

densityplot(~ wave, data = DF_sub, groups = case)
densityplot(~ age, data = DF_sub, groups = case)

tbl1=table(DF_sub$sex,DF_sub$case) 
tbl2=table(DF_sub$region,DF_sub$case)

prop.table(tbl1, 2)
prop.table(tbl2, 2)
```

